{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sktime\n",
    "from sktime.datatypes._panel._convert import from_3d_numpy_to_nested\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reconstructed_data.csv') # load the data of EMG signals\n",
    "\n",
    "df.set_index('sequence',inplace=True) # set the index to the sequence column\n",
    "\n",
    "X = df.drop('label',axis=1) # drop the label column from the data\n",
    "Y = df[['label']] # get the label column to be used as the target\n",
    "#Y = df.drop(df.columns[df.apply(lambda x: x.name != 'label')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14990</th>\n",
       "      <th>14991</th>\n",
       "      <th>14992</th>\n",
       "      <th>14993</th>\n",
       "      <th>14994</th>\n",
       "      <th>14995</th>\n",
       "      <th>14996</th>\n",
       "      <th>14997</th>\n",
       "      <th>14998</th>\n",
       "      <th>14999</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100343</td>\n",
       "      <td>0.100350</td>\n",
       "      <td>0.100370</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.100435</td>\n",
       "      <td>0.100473</td>\n",
       "      <td>0.100511</td>\n",
       "      <td>0.100550</td>\n",
       "      <td>0.100588</td>\n",
       "      <td>0.100627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099908</td>\n",
       "      <td>0.099945</td>\n",
       "      <td>0.099982</td>\n",
       "      <td>0.100019</td>\n",
       "      <td>0.100056</td>\n",
       "      <td>0.100093</td>\n",
       "      <td>0.100129</td>\n",
       "      <td>0.100161</td>\n",
       "      <td>0.100185</td>\n",
       "      <td>0.100198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.110339</td>\n",
       "      <td>0.110336</td>\n",
       "      <td>0.110327</td>\n",
       "      <td>0.110314</td>\n",
       "      <td>0.110298</td>\n",
       "      <td>0.110281</td>\n",
       "      <td>0.110264</td>\n",
       "      <td>0.110247</td>\n",
       "      <td>0.110230</td>\n",
       "      <td>0.110213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110540</td>\n",
       "      <td>0.110523</td>\n",
       "      <td>0.110505</td>\n",
       "      <td>0.110488</td>\n",
       "      <td>0.110471</td>\n",
       "      <td>0.110454</td>\n",
       "      <td>0.110437</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>0.110412</td>\n",
       "      <td>0.110405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036453</td>\n",
       "      <td>0.036453</td>\n",
       "      <td>0.036452</td>\n",
       "      <td>0.036451</td>\n",
       "      <td>0.036449</td>\n",
       "      <td>0.036447</td>\n",
       "      <td>0.036446</td>\n",
       "      <td>0.036444</td>\n",
       "      <td>0.036442</td>\n",
       "      <td>0.036440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036475</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>0.036469</td>\n",
       "      <td>0.036467</td>\n",
       "      <td>0.036466</td>\n",
       "      <td>0.036464</td>\n",
       "      <td>0.036462</td>\n",
       "      <td>0.036461</td>\n",
       "      <td>0.036460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020544</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.020549</td>\n",
       "      <td>0.020556</td>\n",
       "      <td>0.020563</td>\n",
       "      <td>0.020571</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.020588</td>\n",
       "      <td>0.020596</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>0.020463</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>0.020488</td>\n",
       "      <td>0.020496</td>\n",
       "      <td>0.020503</td>\n",
       "      <td>0.020509</td>\n",
       "      <td>0.020512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.103840</td>\n",
       "      <td>0.103843</td>\n",
       "      <td>0.103849</td>\n",
       "      <td>0.103859</td>\n",
       "      <td>0.103871</td>\n",
       "      <td>0.103883</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.103909</td>\n",
       "      <td>0.103922</td>\n",
       "      <td>0.103935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103698</td>\n",
       "      <td>0.103710</td>\n",
       "      <td>0.103722</td>\n",
       "      <td>0.103734</td>\n",
       "      <td>0.103746</td>\n",
       "      <td>0.103758</td>\n",
       "      <td>0.103770</td>\n",
       "      <td>0.103780</td>\n",
       "      <td>0.103788</td>\n",
       "      <td>0.103792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.014313</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.014318</td>\n",
       "      <td>0.014323</td>\n",
       "      <td>0.014329</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>0.014343</td>\n",
       "      <td>0.014350</td>\n",
       "      <td>0.014357</td>\n",
       "      <td>0.014364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014234</td>\n",
       "      <td>0.014241</td>\n",
       "      <td>0.014248</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.014268</td>\n",
       "      <td>0.014274</td>\n",
       "      <td>0.014280</td>\n",
       "      <td>0.014285</td>\n",
       "      <td>0.014287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.093110</td>\n",
       "      <td>0.093094</td>\n",
       "      <td>0.093051</td>\n",
       "      <td>0.092987</td>\n",
       "      <td>0.092911</td>\n",
       "      <td>0.092829</td>\n",
       "      <td>0.092746</td>\n",
       "      <td>0.092663</td>\n",
       "      <td>0.092580</td>\n",
       "      <td>0.092497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094083</td>\n",
       "      <td>0.093999</td>\n",
       "      <td>0.093915</td>\n",
       "      <td>0.093831</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.093663</td>\n",
       "      <td>0.093584</td>\n",
       "      <td>0.093513</td>\n",
       "      <td>0.093459</td>\n",
       "      <td>0.093430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.118611</td>\n",
       "      <td>0.118603</td>\n",
       "      <td>0.118581</td>\n",
       "      <td>0.118549</td>\n",
       "      <td>0.118510</td>\n",
       "      <td>0.118469</td>\n",
       "      <td>0.118427</td>\n",
       "      <td>0.118385</td>\n",
       "      <td>0.118343</td>\n",
       "      <td>0.118301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119101</td>\n",
       "      <td>0.119059</td>\n",
       "      <td>0.119016</td>\n",
       "      <td>0.118974</td>\n",
       "      <td>0.118932</td>\n",
       "      <td>0.118890</td>\n",
       "      <td>0.118850</td>\n",
       "      <td>0.118814</td>\n",
       "      <td>0.118787</td>\n",
       "      <td>0.118772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.041952</td>\n",
       "      <td>0.041949</td>\n",
       "      <td>0.041943</td>\n",
       "      <td>0.041933</td>\n",
       "      <td>0.041921</td>\n",
       "      <td>0.041908</td>\n",
       "      <td>0.041896</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>0.041870</td>\n",
       "      <td>0.041857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042102</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>0.042076</td>\n",
       "      <td>0.042063</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.042025</td>\n",
       "      <td>0.042014</td>\n",
       "      <td>0.042006</td>\n",
       "      <td>0.042001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.011172</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>0.011183</td>\n",
       "      <td>0.011188</td>\n",
       "      <td>0.011193</td>\n",
       "      <td>0.011199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>0.011138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 15000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5  \\\n",
       "sequence                                                               \n",
       "0         0.100343  0.100350  0.100370  0.100400  0.100435  0.100473   \n",
       "0         0.110339  0.110336  0.110327  0.110314  0.110298  0.110281   \n",
       "0         0.036453  0.036453  0.036452  0.036451  0.036449  0.036447   \n",
       "0         0.020544  0.020545  0.020549  0.020556  0.020563  0.020571   \n",
       "1         0.103840  0.103843  0.103849  0.103859  0.103871  0.103883   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1998      0.014313  0.014314  0.014318  0.014323  0.014329  0.014336   \n",
       "1999      0.093110  0.093094  0.093051  0.092987  0.092911  0.092829   \n",
       "1999      0.118611  0.118603  0.118581  0.118549  0.118510  0.118469   \n",
       "1999      0.041952  0.041949  0.041943  0.041933  0.041921  0.041908   \n",
       "1999      0.011159  0.011160  0.011163  0.011167  0.011172  0.011177   \n",
       "\n",
       "                 6         7         8         9  ...     14990     14991  \\\n",
       "sequence                                          ...                       \n",
       "0         0.100511  0.100550  0.100588  0.100627  ...  0.099908  0.099945   \n",
       "0         0.110264  0.110247  0.110230  0.110213  ...  0.110540  0.110523   \n",
       "0         0.036446  0.036444  0.036442  0.036440  ...  0.036475  0.036473   \n",
       "0         0.020580  0.020588  0.020596  0.020604  ...  0.020446  0.020455   \n",
       "1         0.103896  0.103909  0.103922  0.103935  ...  0.103698  0.103710   \n",
       "...            ...       ...       ...       ...  ...       ...       ...   \n",
       "1998      0.014343  0.014350  0.014357  0.014364  ...  0.014234  0.014241   \n",
       "1999      0.092746  0.092663  0.092580  0.092497  ...  0.094083  0.093999   \n",
       "1999      0.118427  0.118385  0.118343  0.118301  ...  0.119101  0.119059   \n",
       "1999      0.041896  0.041883  0.041870  0.041857  ...  0.042102  0.042089   \n",
       "1999      0.011183  0.011188  0.011193  0.011199  ...  0.011096  0.011102   \n",
       "\n",
       "             14992     14993     14994     14995     14996     14997  \\\n",
       "sequence                                                               \n",
       "0         0.099982  0.100019  0.100056  0.100093  0.100129  0.100161   \n",
       "0         0.110505  0.110488  0.110471  0.110454  0.110437  0.110423   \n",
       "0         0.036471  0.036469  0.036467  0.036466  0.036464  0.036462   \n",
       "0         0.020463  0.020472  0.020480  0.020488  0.020496  0.020503   \n",
       "1         0.103722  0.103734  0.103746  0.103758  0.103770  0.103780   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1998      0.014248  0.014254  0.014261  0.014268  0.014274  0.014280   \n",
       "1999      0.093915  0.093831  0.093747  0.093663  0.093584  0.093513   \n",
       "1999      0.119016  0.118974  0.118932  0.118890  0.118850  0.118814   \n",
       "1999      0.042076  0.042063  0.042050  0.042037  0.042025  0.042014   \n",
       "1999      0.011107  0.011113  0.011118  0.011123  0.011128  0.011133   \n",
       "\n",
       "             14998     14999  \n",
       "sequence                      \n",
       "0         0.100185  0.100198  \n",
       "0         0.110412  0.110405  \n",
       "0         0.036461  0.036460  \n",
       "0         0.020509  0.020512  \n",
       "1         0.103788  0.103792  \n",
       "...            ...       ...  \n",
       "1998      0.014285  0.014287  \n",
       "1999      0.093459  0.093430  \n",
       "1999      0.118787  0.118772  \n",
       "1999      0.042006  0.042001  \n",
       "1999      0.011137  0.011138  \n",
       "\n",
       "[8000 rows x 15000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label\n",
       "sequence       \n",
       "0             0\n",
       "0             0\n",
       "0             0\n",
       "0             0\n",
       "1             0\n",
       "...         ...\n",
       "1998          9\n",
       "1999          9\n",
       "1999          9\n",
       "1999          9\n",
       "1999          9\n",
       "\n",
       "[8000 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.10034339, 0.10035037, 0.10037013, ..., 0.10016064,\n",
       "         0.10018479, 0.10019794],\n",
       "        [0.11033939, 0.11033625, 0.11032735, ..., 0.11042262,\n",
       "         0.11041153, 0.1104055 ],\n",
       "        [0.03645344, 0.03645311, 0.03645218, ..., 0.03646228,\n",
       "         0.03646109, 0.03646044],\n",
       "        [0.02054354, 0.02054505, 0.02054933, ..., 0.02050337,\n",
       "         0.02050873, 0.02051165]],\n",
       "\n",
       "       [[0.10384047, 0.10384279, 0.10384935, ..., 0.10378019,\n",
       "         0.10378812, 0.10379244],\n",
       "        [0.10463462, 0.10463483, 0.10463543, ..., 0.10462849,\n",
       "         0.10462936, 0.10462983],\n",
       "        [0.03283462, 0.03283406, 0.03283249, ..., 0.03285   ,\n",
       "         0.03284789, 0.03284675],\n",
       "        [0.01517424, 0.01517493, 0.0151769 , ..., 0.01515572,\n",
       "         0.0151582 , 0.01515955]],\n",
       "\n",
       "       [[0.1147981 , 0.11480091, 0.11480885, ..., 0.11472528,\n",
       "         0.11473484, 0.11474005],\n",
       "        [0.10165947, 0.10165994, 0.10166129, ..., 0.10164754,\n",
       "         0.10164906, 0.1016499 ],\n",
       "        [0.0358137 , 0.03581332, 0.03581226, ..., 0.03582389,\n",
       "         0.03582251, 0.03582176],\n",
       "        [0.01526384, 0.01526308, 0.01526092, ..., 0.01528411,\n",
       "         0.0152814 , 0.01527993]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.10970057, 0.10969043, 0.10966176, ..., 0.10996812,\n",
       "         0.10993254, 0.10991317],\n",
       "        [0.12567961, 0.12567107, 0.12564692, ..., 0.12590625,\n",
       "         0.12587599, 0.12585953],\n",
       "        [0.04245646, 0.04245589, 0.0424543 , ..., 0.04247145,\n",
       "         0.04246944, 0.04246835],\n",
       "        [0.01623784, 0.01623797, 0.01623836, ..., 0.01623427,\n",
       "         0.01623474, 0.01623499]],\n",
       "\n",
       "       [[0.11712523, 0.11711563, 0.11708849, ..., 0.11737937,\n",
       "         0.11734549, 0.11732706],\n",
       "        [0.144929  , 0.1449298 , 0.14493205, ..., 0.1449064 ,\n",
       "         0.14490955, 0.14491126],\n",
       "        [0.04617826, 0.04617944, 0.04618276, ..., 0.04614737,\n",
       "         0.04615146, 0.04615369],\n",
       "        [0.01431308, 0.01431433, 0.01431786, ..., 0.01428016,\n",
       "         0.01428453, 0.01428691]],\n",
       "\n",
       "       [[0.09310956, 0.09309435, 0.09305133, ..., 0.09351285,\n",
       "         0.09345904, 0.09342977],\n",
       "        [0.11861057, 0.11860289, 0.11858116, ..., 0.11881401,\n",
       "         0.11878689, 0.11877213],\n",
       "        [0.04195174, 0.04194939, 0.04194275, ..., 0.04201407,\n",
       "         0.04200574, 0.04200121],\n",
       "        [0.01115908, 0.01116006, 0.01116285, ..., 0.01113303,\n",
       "         0.0111365 , 0.01113839]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xarray = X.to_numpy() # convert the data to numpy array\n",
    "Xarray = Xarray.reshape(2000,4,-1) # reshape the data to 3D numpy array\n",
    "Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yarray = Y.to_numpy() # convert the target to numpy array\n",
    "Yarray = Yarray.reshape(2000,4,-1) # reshape the target to 3D numpy array\n",
    "Yarray = np.mean(Yarray,axis=1).astype(int) # get the mean of the target and convert it to integer to make it a 2D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Xarray, Yarray, test_size = 0.3, random_state=3) # split the data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1400, 1), (600, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, Y_test.shape # check the shape of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "rocket = Rocket(num_kernels=1000,n_jobs=-1) # create a rocket object. set the number of kernels to 1000(default is 10000) and use all available cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "X_train = from_3d_numpy_to_nested(X_train) # convert the training data to nested format\n",
    "rocket.fit(X_train, Y_train) # fit the rocket object to the training data\n",
    "X_train = rocket.transform(X_train) # transform the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = from_3d_numpy_to_nested(X_test) # convert the testing data to nested format\n",
    "X_test = rocket.transform(X_test) # transform the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1400, 2000), (600, 2000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:1190: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86        67\n",
      "           1       0.95      0.85      0.90        62\n",
      "           2       0.96      0.96      0.96        57\n",
      "           3       0.75      0.83      0.79        58\n",
      "           4       0.91      0.81      0.86        75\n",
      "           5       0.90      0.93      0.91        56\n",
      "           6       0.85      0.93      0.89        55\n",
      "           7       0.72      0.79      0.75        61\n",
      "           8       0.68      0.77      0.72        47\n",
      "           9       0.94      0.73      0.82        62\n",
      "\n",
      "    accuracy                           0.85       600\n",
      "   macro avg       0.85      0.85      0.85       600\n",
      "weighted avg       0.85      0.85      0.85       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "clf1 = RidgeClassifierCV() # create a RidgeClassifier object\n",
    "clf1.fit(X_train, Y_train) # fit the classifier with the training data\n",
    "\n",
    "Y_pred = clf1.predict(X_test) # predict the target of the testing data\n",
    "print(classification_report(Y_pred,Y_test)) # print the classification report to check the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91        78\n",
      "           1       0.98      0.87      0.92        63\n",
      "           2       0.81      0.88      0.84        52\n",
      "           3       0.69      0.77      0.73        57\n",
      "           4       0.84      0.79      0.81        71\n",
      "           5       0.91      0.91      0.91        58\n",
      "           6       0.72      0.90      0.80        48\n",
      "           7       0.76      0.85      0.80        60\n",
      "           8       0.74      0.80      0.76        49\n",
      "           9       0.96      0.72      0.82        64\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.84      0.84      0.83       600\n",
      "weighted avg       0.85      0.83      0.84       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier(n_estimators=100) # create a RandomForestClassifier object\n",
    "clf2.fit(X_train,Y_train) # fit the classifier with the training data\n",
    "\n",
    "Y_pred = clf2.predict(X_test) # predict the target of the testing data\n",
    "print(classification_report(Y_pred,Y_test)) # print the classification report to check the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.81        74\n",
      "           1       0.86      0.87      0.86        55\n",
      "           2       0.89      0.98      0.94        52\n",
      "           3       0.72      0.71      0.71        65\n",
      "           4       0.88      0.79      0.83        75\n",
      "           5       0.93      0.95      0.94        57\n",
      "           6       0.82      0.80      0.81        61\n",
      "           7       0.66      0.71      0.68        62\n",
      "           8       0.60      0.74      0.67        43\n",
      "           9       0.77      0.66      0.71        56\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.80      0.80      0.80       600\n",
      "weighted avg       0.80      0.80      0.80       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf3 = SVC(kernel='linear')\n",
    "clf3.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf3.predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:424: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.89      0.80        57\n",
      "           1       0.82      0.81      0.81        57\n",
      "           2       0.75      0.80      0.77        54\n",
      "           3       0.58      0.54      0.56        69\n",
      "           4       0.81      0.70      0.75        77\n",
      "           5       0.76      0.81      0.79        54\n",
      "           6       0.63      0.79      0.70        48\n",
      "           7       0.55      0.59      0.57        63\n",
      "           8       0.58      0.53      0.56        58\n",
      "           9       0.75      0.57      0.65        63\n",
      "\n",
      "    accuracy                           0.69       600\n",
      "   macro avg       0.70      0.70      0.70       600\n",
      "weighted avg       0.70      0.69      0.69       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf4.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf4.predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91        78\n",
      "           1       0.98      0.90      0.94        61\n",
      "           2       0.91      0.98      0.95        53\n",
      "           3       0.83      0.84      0.83        63\n",
      "           4       0.88      0.87      0.87        68\n",
      "           5       0.93      0.92      0.92        59\n",
      "           6       0.73      0.90      0.81        49\n",
      "           7       0.75      0.81      0.78        62\n",
      "           8       0.75      0.83      0.79        48\n",
      "           9       0.90      0.73      0.80        59\n",
      "\n",
      "    accuracy                           0.86       600\n",
      "   macro avg       0.86      0.86      0.86       600\n",
      "weighted avg       0.87      0.86      0.86       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf5 = XGBClassifier()\n",
    "clf5.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf5.predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 433099\n",
      "[LightGBM] [Info] Number of data points in the train set: 1400, number of used features: 1879\n",
      "[LightGBM] [Info] Start training from score -2.376693\n",
      "[LightGBM] [Info] Start training from score -2.274414\n",
      "[LightGBM] [Info] Start training from score -2.281383\n",
      "[LightGBM] [Info] Start training from score -2.331573\n",
      "[LightGBM] [Info] Start training from score -2.353878\n",
      "[LightGBM] [Info] Start training from score -2.288400\n",
      "[LightGBM] [Info] Start training from score -2.302585\n",
      "[LightGBM] [Info] Start training from score -2.353878\n",
      "[LightGBM] [Info] Start training from score -2.253795\n",
      "[LightGBM] [Info] Start training from score -2.220347\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "clf6 = LGBMClassifier()\n",
    "clf6.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89        81\n",
      "           1       0.98      0.86      0.92        64\n",
      "           2       0.93      0.95      0.94        56\n",
      "           3       0.75      0.87      0.81        55\n",
      "           4       0.88      0.81      0.84        73\n",
      "           5       0.93      0.93      0.93        58\n",
      "           6       0.77      0.92      0.84        50\n",
      "           7       0.70      0.82      0.76        57\n",
      "           8       0.74      0.81      0.77        48\n",
      "           9       0.92      0.76      0.83        58\n",
      "\n",
      "    accuracy                           0.85       600\n",
      "   macro avg       0.86      0.86      0.85       600\n",
      "weighted avg       0.86      0.85      0.86       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.18      0.13        38\n",
      "           1       0.32      0.26      0.29        70\n",
      "           2       0.19      0.15      0.17        75\n",
      "           3       0.16      0.13      0.14        79\n",
      "           4       0.27      0.21      0.24        84\n",
      "           5       0.22      0.17      0.20        75\n",
      "           6       0.13      0.24      0.17        34\n",
      "           7       0.13      0.19      0.16        47\n",
      "           8       0.09      0.10      0.10        51\n",
      "           9       0.17      0.17      0.17        47\n",
      "\n",
      "    accuracy                           0.18       600\n",
      "   macro avg       0.18      0.18      0.17       600\n",
      "weighted avg       0.19      0.18      0.18       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "clf7 = QuadraticDiscriminantAnalysis()\n",
    "clf7.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf7.predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.16      0.23       188\n",
      "           1       0.20      0.33      0.25        33\n",
      "           2       0.46      0.14      0.22       181\n",
      "           3       0.06      0.12      0.08        34\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.26      0.16      0.20        91\n",
      "           6       0.02      0.25      0.03         4\n",
      "           7       0.01      0.25      0.03         4\n",
      "           8       0.11      0.20      0.14        30\n",
      "           9       0.10      0.15      0.12        33\n",
      "\n",
      "    accuracy                           0.17       600\n",
      "   macro avg       0.17      0.18      0.13       600\n",
      "weighted avg       0.34      0.17      0.20       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf8 = LinearDiscriminantAnalysis()\n",
    "clf8.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf8.predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        70\n",
      "           1       0.89      0.94      0.92        53\n",
      "           2       0.88      1.00      0.93        50\n",
      "           3       0.66      0.84      0.74        50\n",
      "           4       0.93      0.70      0.79        89\n",
      "           5       0.97      0.88      0.92        64\n",
      "           6       0.83      0.79      0.81        63\n",
      "           7       0.81      0.72      0.76        75\n",
      "           8       0.60      0.86      0.71        37\n",
      "           9       0.77      0.76      0.76        49\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.82      0.84      0.82       600\n",
      "weighted avg       0.84      0.82      0.82       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf9 = MLPClassifier(hidden_layer_sizes=(100,50), max_iter=1000)\n",
    "clf9.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf9.predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigdata/lib/python3.12/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80        67\n",
      "           1       0.93      0.83      0.87        63\n",
      "           2       0.84      0.98      0.91        49\n",
      "           3       0.73      0.77      0.75        61\n",
      "           4       0.87      0.83      0.85        70\n",
      "           5       0.88      0.93      0.90        55\n",
      "           6       0.90      0.76      0.82        71\n",
      "           7       0.72      0.73      0.72        66\n",
      "           8       0.58      0.86      0.70        36\n",
      "           9       0.85      0.66      0.75        62\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.81      0.82      0.81       600\n",
      "weighted avg       0.82      0.81      0.81       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "clf10 = PassiveAggressiveClassifier()\n",
    "clf10.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf10.predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.080496\n",
      "0:\tlearn: 2.1856477\ttotal: 958ms\tremaining: 15m 57s\n",
      "1:\tlearn: 2.0874914\ttotal: 1.75s\tremaining: 14m 31s\n",
      "2:\tlearn: 1.9910772\ttotal: 2.45s\tremaining: 13m 35s\n",
      "3:\tlearn: 1.9128123\ttotal: 3.29s\tremaining: 13m 38s\n",
      "4:\tlearn: 1.8376708\ttotal: 4.13s\tremaining: 13m 42s\n",
      "5:\tlearn: 1.7646793\ttotal: 4.85s\tremaining: 13m 23s\n",
      "6:\tlearn: 1.6970290\ttotal: 5.67s\tremaining: 13m 24s\n",
      "7:\tlearn: 1.6327707\ttotal: 6.4s\tremaining: 13m 13s\n",
      "8:\tlearn: 1.5631452\ttotal: 7.12s\tremaining: 13m 3s\n",
      "9:\tlearn: 1.5092648\ttotal: 7.88s\tremaining: 13m\n",
      "10:\tlearn: 1.4563799\ttotal: 8.66s\tremaining: 12m 58s\n",
      "11:\tlearn: 1.4113627\ttotal: 9.39s\tremaining: 12m 53s\n",
      "12:\tlearn: 1.3665723\ttotal: 10.1s\tremaining: 12m 49s\n",
      "13:\tlearn: 1.3269601\ttotal: 10.8s\tremaining: 12m 42s\n",
      "14:\tlearn: 1.2904288\ttotal: 11.5s\tremaining: 12m 37s\n",
      "15:\tlearn: 1.2575229\ttotal: 12.3s\tremaining: 12m 34s\n",
      "16:\tlearn: 1.2260619\ttotal: 13s\tremaining: 12m 30s\n",
      "17:\tlearn: 1.1939209\ttotal: 13.7s\tremaining: 12m 26s\n",
      "18:\tlearn: 1.1610965\ttotal: 14.4s\tremaining: 12m 24s\n",
      "19:\tlearn: 1.1358570\ttotal: 15.1s\tremaining: 12m 21s\n",
      "20:\tlearn: 1.1099962\ttotal: 15.9s\tremaining: 12m 19s\n",
      "21:\tlearn: 1.0854210\ttotal: 16.6s\tremaining: 12m 16s\n",
      "22:\tlearn: 1.0611808\ttotal: 17.3s\tremaining: 12m 12s\n",
      "23:\tlearn: 1.0405781\ttotal: 17.9s\tremaining: 12m 9s\n",
      "24:\tlearn: 1.0188036\ttotal: 18.6s\tremaining: 12m 6s\n",
      "25:\tlearn: 0.9970423\ttotal: 19.3s\tremaining: 12m 4s\n",
      "26:\tlearn: 0.9782692\ttotal: 20.1s\tremaining: 12m 2s\n",
      "27:\tlearn: 0.9595090\ttotal: 20.8s\tremaining: 12m 1s\n",
      "28:\tlearn: 0.9410076\ttotal: 21.5s\tremaining: 11m 59s\n",
      "29:\tlearn: 0.9256830\ttotal: 22.2s\tremaining: 11m 57s\n",
      "30:\tlearn: 0.9097788\ttotal: 22.9s\tremaining: 11m 54s\n",
      "31:\tlearn: 0.8910321\ttotal: 23.6s\tremaining: 11m 53s\n",
      "32:\tlearn: 0.8757243\ttotal: 24.3s\tremaining: 11m 51s\n",
      "33:\tlearn: 0.8616847\ttotal: 25s\tremaining: 11m 49s\n",
      "34:\tlearn: 0.8493369\ttotal: 25.7s\tremaining: 11m 48s\n",
      "35:\tlearn: 0.8364303\ttotal: 26.6s\tremaining: 11m 51s\n",
      "36:\tlearn: 0.8227793\ttotal: 27.3s\tremaining: 11m 50s\n",
      "37:\tlearn: 0.8085872\ttotal: 28s\tremaining: 11m 49s\n",
      "38:\tlearn: 0.7974823\ttotal: 28.7s\tremaining: 11m 47s\n",
      "39:\tlearn: 0.7850509\ttotal: 29.4s\tremaining: 11m 46s\n",
      "40:\tlearn: 0.7719971\ttotal: 30.1s\tremaining: 11m 44s\n",
      "41:\tlearn: 0.7607186\ttotal: 30.9s\tremaining: 11m 43s\n",
      "42:\tlearn: 0.7507848\ttotal: 31.6s\tremaining: 11m 42s\n",
      "43:\tlearn: 0.7408644\ttotal: 32.3s\tremaining: 11m 41s\n",
      "44:\tlearn: 0.7332217\ttotal: 33s\tremaining: 11m 40s\n",
      "45:\tlearn: 0.7222799\ttotal: 33.7s\tremaining: 11m 38s\n",
      "46:\tlearn: 0.7105726\ttotal: 34.4s\tremaining: 11m 37s\n",
      "47:\tlearn: 0.7025032\ttotal: 35.1s\tremaining: 11m 35s\n",
      "48:\tlearn: 0.6935688\ttotal: 35.8s\tremaining: 11m 33s\n",
      "49:\tlearn: 0.6852485\ttotal: 36.5s\tremaining: 11m 32s\n",
      "50:\tlearn: 0.6756995\ttotal: 37.2s\tremaining: 11m 31s\n",
      "51:\tlearn: 0.6674549\ttotal: 37.9s\tremaining: 11m 30s\n",
      "52:\tlearn: 0.6594671\ttotal: 38.6s\tremaining: 11m 29s\n",
      "53:\tlearn: 0.6505601\ttotal: 39.3s\tremaining: 11m 28s\n",
      "54:\tlearn: 0.6425291\ttotal: 40s\tremaining: 11m 27s\n",
      "55:\tlearn: 0.6339510\ttotal: 40.7s\tremaining: 11m 25s\n",
      "56:\tlearn: 0.6256151\ttotal: 41.4s\tremaining: 11m 24s\n",
      "57:\tlearn: 0.6174246\ttotal: 42.1s\tremaining: 11m 23s\n",
      "58:\tlearn: 0.6110472\ttotal: 42.8s\tremaining: 11m 22s\n",
      "59:\tlearn: 0.6048279\ttotal: 43.5s\tremaining: 11m 21s\n",
      "60:\tlearn: 0.5974131\ttotal: 44.2s\tremaining: 11m 20s\n",
      "61:\tlearn: 0.5912419\ttotal: 44.9s\tremaining: 11m 19s\n",
      "62:\tlearn: 0.5846286\ttotal: 45.6s\tremaining: 11m 18s\n",
      "63:\tlearn: 0.5777409\ttotal: 46.3s\tremaining: 11m 16s\n",
      "64:\tlearn: 0.5718633\ttotal: 47s\tremaining: 11m 15s\n",
      "65:\tlearn: 0.5647749\ttotal: 47.7s\tremaining: 11m 14s\n",
      "66:\tlearn: 0.5585023\ttotal: 48.4s\tremaining: 11m 13s\n",
      "67:\tlearn: 0.5517597\ttotal: 49.1s\tremaining: 11m 12s\n",
      "68:\tlearn: 0.5462732\ttotal: 49.8s\tremaining: 11m 11s\n",
      "69:\tlearn: 0.5386751\ttotal: 50.5s\tremaining: 11m 10s\n",
      "70:\tlearn: 0.5337188\ttotal: 51.2s\tremaining: 11m 9s\n",
      "71:\tlearn: 0.5276340\ttotal: 51.9s\tremaining: 11m 8s\n",
      "72:\tlearn: 0.5222931\ttotal: 52.6s\tremaining: 11m 7s\n",
      "73:\tlearn: 0.5170969\ttotal: 53.2s\tremaining: 11m 6s\n",
      "74:\tlearn: 0.5120658\ttotal: 54s\tremaining: 11m 5s\n",
      "75:\tlearn: 0.5063598\ttotal: 54.7s\tremaining: 11m 4s\n",
      "76:\tlearn: 0.5004204\ttotal: 55.4s\tremaining: 11m 4s\n",
      "77:\tlearn: 0.4945255\ttotal: 56.1s\tremaining: 11m 3s\n",
      "78:\tlearn: 0.4903435\ttotal: 56.8s\tremaining: 11m 1s\n",
      "79:\tlearn: 0.4866869\ttotal: 57.5s\tremaining: 11m\n",
      "80:\tlearn: 0.4825967\ttotal: 58.1s\tremaining: 10m 59s\n",
      "81:\tlearn: 0.4772738\ttotal: 58.8s\tremaining: 10m 58s\n",
      "82:\tlearn: 0.4722011\ttotal: 59.5s\tremaining: 10m 57s\n",
      "83:\tlearn: 0.4684040\ttotal: 1m\tremaining: 10m 56s\n",
      "84:\tlearn: 0.4641634\ttotal: 1m\tremaining: 10m 55s\n",
      "85:\tlearn: 0.4602102\ttotal: 1m 1s\tremaining: 10m 55s\n",
      "86:\tlearn: 0.4544896\ttotal: 1m 2s\tremaining: 10m 54s\n",
      "87:\tlearn: 0.4515711\ttotal: 1m 3s\tremaining: 10m 53s\n",
      "88:\tlearn: 0.4472578\ttotal: 1m 3s\tremaining: 10m 52s\n",
      "89:\tlearn: 0.4430048\ttotal: 1m 4s\tremaining: 10m 51s\n",
      "90:\tlearn: 0.4392753\ttotal: 1m 5s\tremaining: 10m 50s\n",
      "91:\tlearn: 0.4364151\ttotal: 1m 5s\tremaining: 10m 49s\n",
      "92:\tlearn: 0.4329598\ttotal: 1m 6s\tremaining: 10m 49s\n",
      "93:\tlearn: 0.4283927\ttotal: 1m 7s\tremaining: 10m 49s\n",
      "94:\tlearn: 0.4232170\ttotal: 1m 8s\tremaining: 10m 49s\n",
      "95:\tlearn: 0.4204427\ttotal: 1m 8s\tremaining: 10m 48s\n",
      "96:\tlearn: 0.4163981\ttotal: 1m 9s\tremaining: 10m 47s\n",
      "97:\tlearn: 0.4136999\ttotal: 1m 10s\tremaining: 10m 46s\n",
      "98:\tlearn: 0.4099739\ttotal: 1m 10s\tremaining: 10m 45s\n",
      "99:\tlearn: 0.4060026\ttotal: 1m 11s\tremaining: 10m 44s\n",
      "100:\tlearn: 0.4024818\ttotal: 1m 12s\tremaining: 10m 43s\n",
      "101:\tlearn: 0.3998301\ttotal: 1m 13s\tremaining: 10m 42s\n",
      "102:\tlearn: 0.3974154\ttotal: 1m 13s\tremaining: 10m 42s\n",
      "103:\tlearn: 0.3948410\ttotal: 1m 14s\tremaining: 10m 41s\n",
      "104:\tlearn: 0.3908526\ttotal: 1m 15s\tremaining: 10m 40s\n",
      "105:\tlearn: 0.3868791\ttotal: 1m 15s\tremaining: 10m 40s\n",
      "106:\tlearn: 0.3823530\ttotal: 1m 16s\tremaining: 10m 39s\n",
      "107:\tlearn: 0.3797260\ttotal: 1m 17s\tremaining: 10m 38s\n",
      "108:\tlearn: 0.3765960\ttotal: 1m 17s\tremaining: 10m 37s\n",
      "109:\tlearn: 0.3734452\ttotal: 1m 18s\tremaining: 10m 36s\n",
      "110:\tlearn: 0.3691242\ttotal: 1m 19s\tremaining: 10m 35s\n",
      "111:\tlearn: 0.3661705\ttotal: 1m 20s\tremaining: 10m 34s\n",
      "112:\tlearn: 0.3636051\ttotal: 1m 20s\tremaining: 10m 33s\n",
      "113:\tlearn: 0.3598736\ttotal: 1m 21s\tremaining: 10m 32s\n",
      "114:\tlearn: 0.3575596\ttotal: 1m 22s\tremaining: 10m 31s\n",
      "115:\tlearn: 0.3547717\ttotal: 1m 22s\tremaining: 10m 31s\n",
      "116:\tlearn: 0.3517121\ttotal: 1m 23s\tremaining: 10m 30s\n",
      "117:\tlearn: 0.3500887\ttotal: 1m 24s\tremaining: 10m 29s\n",
      "118:\tlearn: 0.3475734\ttotal: 1m 24s\tremaining: 10m 28s\n",
      "119:\tlearn: 0.3451258\ttotal: 1m 25s\tremaining: 10m 27s\n",
      "120:\tlearn: 0.3427339\ttotal: 1m 26s\tremaining: 10m 26s\n",
      "121:\tlearn: 0.3401206\ttotal: 1m 26s\tremaining: 10m 25s\n",
      "122:\tlearn: 0.3360671\ttotal: 1m 27s\tremaining: 10m 25s\n",
      "123:\tlearn: 0.3339087\ttotal: 1m 28s\tremaining: 10m 24s\n",
      "124:\tlearn: 0.3320648\ttotal: 1m 29s\tremaining: 10m 23s\n",
      "125:\tlearn: 0.3289672\ttotal: 1m 29s\tremaining: 10m 22s\n",
      "126:\tlearn: 0.3270614\ttotal: 1m 30s\tremaining: 10m 21s\n",
      "127:\tlearn: 0.3235661\ttotal: 1m 31s\tremaining: 10m 20s\n",
      "128:\tlearn: 0.3216863\ttotal: 1m 31s\tremaining: 10m 19s\n",
      "129:\tlearn: 0.3199336\ttotal: 1m 32s\tremaining: 10m 18s\n",
      "130:\tlearn: 0.3174735\ttotal: 1m 33s\tremaining: 10m 18s\n",
      "131:\tlearn: 0.3163559\ttotal: 1m 33s\tremaining: 10m 17s\n",
      "132:\tlearn: 0.3143428\ttotal: 1m 34s\tremaining: 10m 16s\n",
      "133:\tlearn: 0.3116006\ttotal: 1m 35s\tremaining: 10m 15s\n",
      "134:\tlearn: 0.3091288\ttotal: 1m 35s\tremaining: 10m 14s\n",
      "135:\tlearn: 0.3067983\ttotal: 1m 36s\tremaining: 10m 13s\n",
      "136:\tlearn: 0.3049479\ttotal: 1m 37s\tremaining: 10m 12s\n",
      "137:\tlearn: 0.3029327\ttotal: 1m 37s\tremaining: 10m 12s\n",
      "138:\tlearn: 0.3011717\ttotal: 1m 38s\tremaining: 10m 11s\n",
      "139:\tlearn: 0.2997838\ttotal: 1m 39s\tremaining: 10m 10s\n",
      "140:\tlearn: 0.2967095\ttotal: 1m 40s\tremaining: 10m 9s\n",
      "141:\tlearn: 0.2946004\ttotal: 1m 40s\tremaining: 10m 8s\n",
      "142:\tlearn: 0.2927798\ttotal: 1m 41s\tremaining: 10m 7s\n",
      "143:\tlearn: 0.2915040\ttotal: 1m 42s\tremaining: 10m 7s\n",
      "144:\tlearn: 0.2899436\ttotal: 1m 42s\tremaining: 10m 6s\n",
      "145:\tlearn: 0.2881741\ttotal: 1m 43s\tremaining: 10m 5s\n",
      "146:\tlearn: 0.2870369\ttotal: 1m 44s\tremaining: 10m 4s\n",
      "147:\tlearn: 0.2859183\ttotal: 1m 44s\tremaining: 10m 3s\n",
      "148:\tlearn: 0.2831261\ttotal: 1m 45s\tremaining: 10m 2s\n",
      "149:\tlearn: 0.2811952\ttotal: 1m 46s\tremaining: 10m 1s\n",
      "150:\tlearn: 0.2793672\ttotal: 1m 46s\tremaining: 10m 1s\n",
      "151:\tlearn: 0.2778339\ttotal: 1m 47s\tremaining: 10m\n",
      "152:\tlearn: 0.2764733\ttotal: 1m 48s\tremaining: 9m 59s\n",
      "153:\tlearn: 0.2748918\ttotal: 1m 49s\tremaining: 9m 58s\n",
      "154:\tlearn: 0.2727256\ttotal: 1m 49s\tremaining: 9m 57s\n",
      "155:\tlearn: 0.2705882\ttotal: 1m 50s\tremaining: 9m 57s\n",
      "156:\tlearn: 0.2700127\ttotal: 1m 51s\tremaining: 9m 56s\n",
      "157:\tlearn: 0.2689695\ttotal: 1m 51s\tremaining: 9m 55s\n",
      "158:\tlearn: 0.2679638\ttotal: 1m 52s\tremaining: 9m 54s\n",
      "159:\tlearn: 0.2673549\ttotal: 1m 53s\tremaining: 9m 53s\n",
      "160:\tlearn: 0.2665025\ttotal: 1m 53s\tremaining: 9m 53s\n",
      "161:\tlearn: 0.2653585\ttotal: 1m 54s\tremaining: 9m 52s\n",
      "162:\tlearn: 0.2648240\ttotal: 1m 55s\tremaining: 9m 52s\n",
      "163:\tlearn: 0.2636912\ttotal: 1m 55s\tremaining: 9m 51s\n",
      "164:\tlearn: 0.2622364\ttotal: 1m 56s\tremaining: 9m 50s\n",
      "165:\tlearn: 0.2614193\ttotal: 1m 57s\tremaining: 9m 49s\n",
      "166:\tlearn: 0.2600346\ttotal: 1m 58s\tremaining: 9m 48s\n",
      "167:\tlearn: 0.2583407\ttotal: 1m 58s\tremaining: 9m 47s\n",
      "168:\tlearn: 0.2566606\ttotal: 1m 59s\tremaining: 9m 47s\n",
      "169:\tlearn: 0.2556788\ttotal: 2m\tremaining: 9m 46s\n",
      "170:\tlearn: 0.2545156\ttotal: 2m\tremaining: 9m 45s\n",
      "171:\tlearn: 0.2530837\ttotal: 2m 1s\tremaining: 9m 44s\n",
      "172:\tlearn: 0.2515301\ttotal: 2m 2s\tremaining: 9m 43s\n",
      "173:\tlearn: 0.2504954\ttotal: 2m 2s\tremaining: 9m 43s\n",
      "174:\tlearn: 0.2497103\ttotal: 2m 3s\tremaining: 9m 42s\n",
      "175:\tlearn: 0.2488477\ttotal: 2m 4s\tremaining: 9m 41s\n",
      "176:\tlearn: 0.2469521\ttotal: 2m 4s\tremaining: 9m 40s\n",
      "177:\tlearn: 0.2462431\ttotal: 2m 5s\tremaining: 9m 40s\n",
      "178:\tlearn: 0.2448544\ttotal: 2m 6s\tremaining: 9m 39s\n",
      "179:\tlearn: 0.2437410\ttotal: 2m 6s\tremaining: 9m 38s\n",
      "180:\tlearn: 0.2426263\ttotal: 2m 7s\tremaining: 9m 37s\n",
      "181:\tlearn: 0.2413567\ttotal: 2m 8s\tremaining: 9m 37s\n",
      "182:\tlearn: 0.2407341\ttotal: 2m 9s\tremaining: 9m 36s\n",
      "183:\tlearn: 0.2395212\ttotal: 2m 9s\tremaining: 9m 35s\n",
      "184:\tlearn: 0.2389806\ttotal: 2m 10s\tremaining: 9m 34s\n",
      "185:\tlearn: 0.2384155\ttotal: 2m 11s\tremaining: 9m 33s\n",
      "186:\tlearn: 0.2376702\ttotal: 2m 11s\tremaining: 9m 33s\n",
      "187:\tlearn: 0.2371285\ttotal: 2m 12s\tremaining: 9m 32s\n",
      "188:\tlearn: 0.2355736\ttotal: 2m 13s\tremaining: 9m 31s\n",
      "189:\tlearn: 0.2350555\ttotal: 2m 13s\tremaining: 9m 30s\n",
      "190:\tlearn: 0.2331649\ttotal: 2m 14s\tremaining: 9m 29s\n",
      "191:\tlearn: 0.2316697\ttotal: 2m 15s\tremaining: 9m 29s\n",
      "192:\tlearn: 0.2307895\ttotal: 2m 15s\tremaining: 9m 28s\n",
      "193:\tlearn: 0.2294068\ttotal: 2m 16s\tremaining: 9m 27s\n",
      "194:\tlearn: 0.2282760\ttotal: 2m 17s\tremaining: 9m 26s\n",
      "195:\tlearn: 0.2274687\ttotal: 2m 18s\tremaining: 9m 26s\n",
      "196:\tlearn: 0.2270937\ttotal: 2m 18s\tremaining: 9m 25s\n",
      "197:\tlearn: 0.2262677\ttotal: 2m 19s\tremaining: 9m 24s\n",
      "198:\tlearn: 0.2253407\ttotal: 2m 20s\tremaining: 9m 23s\n",
      "199:\tlearn: 0.2244753\ttotal: 2m 20s\tremaining: 9m 22s\n",
      "200:\tlearn: 0.2233026\ttotal: 2m 21s\tremaining: 9m 22s\n",
      "201:\tlearn: 0.2229443\ttotal: 2m 22s\tremaining: 9m 21s\n",
      "202:\tlearn: 0.2212636\ttotal: 2m 22s\tremaining: 9m 20s\n",
      "203:\tlearn: 0.2204626\ttotal: 2m 23s\tremaining: 9m 20s\n",
      "204:\tlearn: 0.2195244\ttotal: 2m 24s\tremaining: 9m 19s\n",
      "205:\tlearn: 0.2181588\ttotal: 2m 24s\tremaining: 9m 18s\n",
      "206:\tlearn: 0.2169659\ttotal: 2m 25s\tremaining: 9m 17s\n",
      "207:\tlearn: 0.2165197\ttotal: 2m 26s\tremaining: 9m 17s\n",
      "208:\tlearn: 0.2156212\ttotal: 2m 26s\tremaining: 9m 16s\n",
      "209:\tlearn: 0.2147060\ttotal: 2m 27s\tremaining: 9m 15s\n",
      "210:\tlearn: 0.2136416\ttotal: 2m 28s\tremaining: 9m 14s\n",
      "211:\tlearn: 0.2131501\ttotal: 2m 29s\tremaining: 9m 14s\n",
      "212:\tlearn: 0.2126312\ttotal: 2m 29s\tremaining: 9m 13s\n",
      "213:\tlearn: 0.2115822\ttotal: 2m 30s\tremaining: 9m 12s\n",
      "214:\tlearn: 0.2108603\ttotal: 2m 31s\tremaining: 9m 11s\n",
      "215:\tlearn: 0.2096579\ttotal: 2m 31s\tremaining: 9m 11s\n",
      "216:\tlearn: 0.2085593\ttotal: 2m 32s\tremaining: 9m 10s\n",
      "217:\tlearn: 0.2075102\ttotal: 2m 33s\tremaining: 9m 9s\n",
      "218:\tlearn: 0.2067626\ttotal: 2m 33s\tremaining: 9m 8s\n",
      "219:\tlearn: 0.2058339\ttotal: 2m 34s\tremaining: 9m 8s\n",
      "220:\tlearn: 0.2050060\ttotal: 2m 35s\tremaining: 9m 7s\n",
      "221:\tlearn: 0.2042456\ttotal: 2m 36s\tremaining: 9m 6s\n",
      "222:\tlearn: 0.2030161\ttotal: 2m 36s\tremaining: 9m 6s\n",
      "223:\tlearn: 0.2023260\ttotal: 2m 37s\tremaining: 9m 5s\n",
      "224:\tlearn: 0.2019867\ttotal: 2m 38s\tremaining: 9m 4s\n",
      "225:\tlearn: 0.2015810\ttotal: 2m 38s\tremaining: 9m 4s\n",
      "226:\tlearn: 0.2004426\ttotal: 2m 39s\tremaining: 9m 3s\n",
      "227:\tlearn: 0.1996344\ttotal: 2m 40s\tremaining: 9m 2s\n",
      "228:\tlearn: 0.1989316\ttotal: 2m 40s\tremaining: 9m 1s\n",
      "229:\tlearn: 0.1986367\ttotal: 2m 41s\tremaining: 9m 1s\n",
      "230:\tlearn: 0.1980203\ttotal: 2m 42s\tremaining: 9m\n",
      "231:\tlearn: 0.1972345\ttotal: 2m 42s\tremaining: 8m 59s\n",
      "232:\tlearn: 0.1963787\ttotal: 2m 43s\tremaining: 8m 58s\n",
      "233:\tlearn: 0.1956464\ttotal: 2m 44s\tremaining: 8m 58s\n",
      "234:\tlearn: 0.1949118\ttotal: 2m 45s\tremaining: 8m 57s\n",
      "235:\tlearn: 0.1938900\ttotal: 2m 45s\tremaining: 8m 56s\n",
      "236:\tlearn: 0.1930647\ttotal: 2m 46s\tremaining: 8m 55s\n",
      "237:\tlearn: 0.1925110\ttotal: 2m 47s\tremaining: 8m 55s\n",
      "238:\tlearn: 0.1922270\ttotal: 2m 47s\tremaining: 8m 54s\n",
      "239:\tlearn: 0.1914621\ttotal: 2m 48s\tremaining: 8m 53s\n",
      "240:\tlearn: 0.1905667\ttotal: 2m 49s\tremaining: 8m 52s\n",
      "241:\tlearn: 0.1901713\ttotal: 2m 49s\tremaining: 8m 51s\n",
      "242:\tlearn: 0.1898814\ttotal: 2m 50s\tremaining: 8m 51s\n",
      "243:\tlearn: 0.1892869\ttotal: 2m 51s\tremaining: 8m 50s\n",
      "244:\tlearn: 0.1890354\ttotal: 2m 51s\tremaining: 8m 50s\n",
      "245:\tlearn: 0.1879619\ttotal: 2m 52s\tremaining: 8m 49s\n",
      "246:\tlearn: 0.1876070\ttotal: 2m 53s\tremaining: 8m 50s\n",
      "247:\tlearn: 0.1863767\ttotal: 2m 54s\tremaining: 8m 49s\n",
      "248:\tlearn: 0.1855167\ttotal: 2m 55s\tremaining: 8m 49s\n",
      "249:\tlearn: 0.1844558\ttotal: 2m 56s\tremaining: 8m 48s\n",
      "250:\tlearn: 0.1840772\ttotal: 2m 56s\tremaining: 8m 47s\n",
      "251:\tlearn: 0.1834761\ttotal: 2m 57s\tremaining: 8m 47s\n",
      "252:\tlearn: 0.1828385\ttotal: 2m 58s\tremaining: 8m 46s\n",
      "253:\tlearn: 0.1821309\ttotal: 2m 59s\tremaining: 8m 46s\n",
      "254:\tlearn: 0.1815202\ttotal: 3m\tremaining: 8m 46s\n",
      "255:\tlearn: 0.1805821\ttotal: 3m 1s\tremaining: 8m 46s\n",
      "256:\tlearn: 0.1801246\ttotal: 3m 1s\tremaining: 8m 46s\n",
      "257:\tlearn: 0.1797565\ttotal: 3m 2s\tremaining: 8m 45s\n",
      "258:\tlearn: 0.1789929\ttotal: 3m 3s\tremaining: 8m 45s\n",
      "259:\tlearn: 0.1780918\ttotal: 3m 4s\tremaining: 8m 45s\n",
      "260:\tlearn: 0.1776409\ttotal: 3m 5s\tremaining: 8m 44s\n",
      "261:\tlearn: 0.1771820\ttotal: 3m 6s\tremaining: 8m 44s\n",
      "262:\tlearn: 0.1765147\ttotal: 3m 6s\tremaining: 8m 43s\n",
      "263:\tlearn: 0.1759234\ttotal: 3m 7s\tremaining: 8m 42s\n",
      "264:\tlearn: 0.1755759\ttotal: 3m 8s\tremaining: 8m 41s\n",
      "265:\tlearn: 0.1748223\ttotal: 3m 8s\tremaining: 8m 41s\n",
      "266:\tlearn: 0.1739444\ttotal: 3m 9s\tremaining: 8m 40s\n",
      "267:\tlearn: 0.1731428\ttotal: 3m 10s\tremaining: 8m 39s\n",
      "268:\tlearn: 0.1723508\ttotal: 3m 10s\tremaining: 8m 38s\n",
      "269:\tlearn: 0.1717104\ttotal: 3m 11s\tremaining: 8m 37s\n",
      "270:\tlearn: 0.1711900\ttotal: 3m 12s\tremaining: 8m 37s\n",
      "271:\tlearn: 0.1707692\ttotal: 3m 12s\tremaining: 8m 36s\n",
      "272:\tlearn: 0.1699677\ttotal: 3m 13s\tremaining: 8m 35s\n",
      "273:\tlearn: 0.1696968\ttotal: 3m 14s\tremaining: 8m 35s\n",
      "274:\tlearn: 0.1694388\ttotal: 3m 15s\tremaining: 8m 34s\n",
      "275:\tlearn: 0.1690166\ttotal: 3m 15s\tremaining: 8m 33s\n",
      "276:\tlearn: 0.1685496\ttotal: 3m 16s\tremaining: 8m 33s\n",
      "277:\tlearn: 0.1680732\ttotal: 3m 17s\tremaining: 8m 32s\n",
      "278:\tlearn: 0.1674229\ttotal: 3m 18s\tremaining: 8m 33s\n",
      "279:\tlearn: 0.1671602\ttotal: 3m 19s\tremaining: 8m 33s\n",
      "280:\tlearn: 0.1662719\ttotal: 3m 20s\tremaining: 8m 32s\n",
      "281:\tlearn: 0.1660798\ttotal: 3m 21s\tremaining: 8m 33s\n",
      "282:\tlearn: 0.1655864\ttotal: 3m 23s\tremaining: 8m 34s\n",
      "283:\tlearn: 0.1653891\ttotal: 3m 24s\tremaining: 8m 34s\n",
      "284:\tlearn: 0.1650590\ttotal: 3m 25s\tremaining: 8m 34s\n",
      "285:\tlearn: 0.1643427\ttotal: 3m 25s\tremaining: 8m 33s\n",
      "286:\tlearn: 0.1640418\ttotal: 3m 26s\tremaining: 8m 32s\n",
      "287:\tlearn: 0.1634775\ttotal: 3m 27s\tremaining: 8m 32s\n",
      "288:\tlearn: 0.1630397\ttotal: 3m 27s\tremaining: 8m 31s\n",
      "289:\tlearn: 0.1623551\ttotal: 3m 28s\tremaining: 8m 30s\n",
      "290:\tlearn: 0.1620297\ttotal: 3m 29s\tremaining: 8m 30s\n",
      "291:\tlearn: 0.1615945\ttotal: 3m 30s\tremaining: 8m 29s\n",
      "292:\tlearn: 0.1605954\ttotal: 3m 31s\tremaining: 8m 29s\n",
      "293:\tlearn: 0.1602223\ttotal: 3m 31s\tremaining: 8m 28s\n",
      "294:\tlearn: 0.1596008\ttotal: 3m 32s\tremaining: 8m 27s\n",
      "295:\tlearn: 0.1590788\ttotal: 3m 33s\tremaining: 8m 26s\n",
      "296:\tlearn: 0.1582641\ttotal: 3m 33s\tremaining: 8m 26s\n",
      "297:\tlearn: 0.1579177\ttotal: 3m 34s\tremaining: 8m 25s\n",
      "298:\tlearn: 0.1575899\ttotal: 3m 35s\tremaining: 8m 24s\n",
      "299:\tlearn: 0.1571556\ttotal: 3m 35s\tremaining: 8m 23s\n",
      "300:\tlearn: 0.1565589\ttotal: 3m 36s\tremaining: 8m 23s\n",
      "301:\tlearn: 0.1562948\ttotal: 3m 37s\tremaining: 8m 22s\n",
      "302:\tlearn: 0.1556613\ttotal: 3m 38s\tremaining: 8m 21s\n",
      "303:\tlearn: 0.1547521\ttotal: 3m 38s\tremaining: 8m 21s\n",
      "304:\tlearn: 0.1545821\ttotal: 3m 39s\tremaining: 8m 20s\n",
      "305:\tlearn: 0.1542166\ttotal: 3m 40s\tremaining: 8m 19s\n",
      "306:\tlearn: 0.1539448\ttotal: 3m 40s\tremaining: 8m 18s\n",
      "307:\tlearn: 0.1532591\ttotal: 3m 41s\tremaining: 8m 18s\n",
      "308:\tlearn: 0.1526878\ttotal: 3m 42s\tremaining: 8m 17s\n",
      "309:\tlearn: 0.1521737\ttotal: 3m 43s\tremaining: 8m 16s\n",
      "310:\tlearn: 0.1511998\ttotal: 3m 43s\tremaining: 8m 15s\n",
      "311:\tlearn: 0.1510156\ttotal: 3m 44s\tremaining: 8m 15s\n",
      "312:\tlearn: 0.1506465\ttotal: 3m 45s\tremaining: 8m 14s\n",
      "313:\tlearn: 0.1498828\ttotal: 3m 45s\tremaining: 8m 13s\n",
      "314:\tlearn: 0.1495127\ttotal: 3m 46s\tremaining: 8m 12s\n",
      "315:\tlearn: 0.1492918\ttotal: 3m 47s\tremaining: 8m 12s\n",
      "316:\tlearn: 0.1491277\ttotal: 3m 47s\tremaining: 8m 11s\n",
      "317:\tlearn: 0.1486810\ttotal: 3m 48s\tremaining: 8m 10s\n",
      "318:\tlearn: 0.1483877\ttotal: 3m 49s\tremaining: 8m 9s\n",
      "319:\tlearn: 0.1479776\ttotal: 3m 50s\tremaining: 8m 8s\n",
      "320:\tlearn: 0.1474819\ttotal: 3m 50s\tremaining: 8m 8s\n",
      "321:\tlearn: 0.1469309\ttotal: 3m 51s\tremaining: 8m 7s\n",
      "322:\tlearn: 0.1467120\ttotal: 3m 52s\tremaining: 8m 6s\n",
      "323:\tlearn: 0.1464724\ttotal: 3m 52s\tremaining: 8m 6s\n",
      "324:\tlearn: 0.1458918\ttotal: 3m 53s\tremaining: 8m 5s\n",
      "325:\tlearn: 0.1454011\ttotal: 3m 54s\tremaining: 8m 4s\n",
      "326:\tlearn: 0.1451074\ttotal: 3m 55s\tremaining: 8m 3s\n",
      "327:\tlearn: 0.1446585\ttotal: 3m 55s\tremaining: 8m 3s\n",
      "328:\tlearn: 0.1437941\ttotal: 3m 56s\tremaining: 8m 2s\n",
      "329:\tlearn: 0.1434282\ttotal: 3m 57s\tremaining: 8m 1s\n",
      "330:\tlearn: 0.1431679\ttotal: 3m 57s\tremaining: 8m\n",
      "331:\tlearn: 0.1424968\ttotal: 3m 58s\tremaining: 8m\n",
      "332:\tlearn: 0.1421359\ttotal: 3m 59s\tremaining: 7m 59s\n",
      "333:\tlearn: 0.1413849\ttotal: 4m\tremaining: 7m 58s\n",
      "334:\tlearn: 0.1406271\ttotal: 4m\tremaining: 7m 58s\n",
      "335:\tlearn: 0.1402847\ttotal: 4m 1s\tremaining: 7m 57s\n",
      "336:\tlearn: 0.1401330\ttotal: 4m 2s\tremaining: 7m 56s\n",
      "337:\tlearn: 0.1398611\ttotal: 4m 2s\tremaining: 7m 55s\n",
      "338:\tlearn: 0.1396272\ttotal: 4m 3s\tremaining: 7m 55s\n",
      "339:\tlearn: 0.1393846\ttotal: 4m 4s\tremaining: 7m 54s\n",
      "340:\tlearn: 0.1390823\ttotal: 4m 5s\tremaining: 7m 53s\n",
      "341:\tlearn: 0.1388538\ttotal: 4m 5s\tremaining: 7m 52s\n",
      "342:\tlearn: 0.1386081\ttotal: 4m 6s\tremaining: 7m 51s\n",
      "343:\tlearn: 0.1383858\ttotal: 4m 7s\tremaining: 7m 51s\n",
      "344:\tlearn: 0.1378205\ttotal: 4m 7s\tremaining: 7m 50s\n",
      "345:\tlearn: 0.1375821\ttotal: 4m 8s\tremaining: 7m 49s\n",
      "346:\tlearn: 0.1371836\ttotal: 4m 9s\tremaining: 7m 48s\n",
      "347:\tlearn: 0.1368619\ttotal: 4m 9s\tremaining: 7m 48s\n",
      "348:\tlearn: 0.1363108\ttotal: 4m 10s\tremaining: 7m 47s\n",
      "349:\tlearn: 0.1358795\ttotal: 4m 11s\tremaining: 7m 46s\n",
      "350:\tlearn: 0.1354461\ttotal: 4m 11s\tremaining: 7m 45s\n",
      "351:\tlearn: 0.1348098\ttotal: 4m 12s\tremaining: 7m 45s\n",
      "352:\tlearn: 0.1342081\ttotal: 4m 13s\tremaining: 7m 44s\n",
      "353:\tlearn: 0.1338491\ttotal: 4m 14s\tremaining: 7m 43s\n",
      "354:\tlearn: 0.1333939\ttotal: 4m 14s\tremaining: 7m 42s\n",
      "355:\tlearn: 0.1329439\ttotal: 4m 15s\tremaining: 7m 42s\n",
      "356:\tlearn: 0.1323889\ttotal: 4m 16s\tremaining: 7m 41s\n",
      "357:\tlearn: 0.1320001\ttotal: 4m 16s\tremaining: 7m 40s\n",
      "358:\tlearn: 0.1315374\ttotal: 4m 17s\tremaining: 7m 39s\n",
      "359:\tlearn: 0.1313657\ttotal: 4m 18s\tremaining: 7m 39s\n",
      "360:\tlearn: 0.1310850\ttotal: 4m 18s\tremaining: 7m 38s\n",
      "361:\tlearn: 0.1308280\ttotal: 4m 19s\tremaining: 7m 37s\n",
      "362:\tlearn: 0.1305527\ttotal: 4m 20s\tremaining: 7m 36s\n",
      "363:\tlearn: 0.1300326\ttotal: 4m 21s\tremaining: 7m 36s\n",
      "364:\tlearn: 0.1295518\ttotal: 4m 21s\tremaining: 7m 35s\n",
      "365:\tlearn: 0.1291003\ttotal: 4m 22s\tremaining: 7m 34s\n",
      "366:\tlearn: 0.1285471\ttotal: 4m 23s\tremaining: 7m 34s\n",
      "367:\tlearn: 0.1283793\ttotal: 4m 24s\tremaining: 7m 33s\n",
      "368:\tlearn: 0.1281300\ttotal: 4m 24s\tremaining: 7m 32s\n",
      "369:\tlearn: 0.1276764\ttotal: 4m 25s\tremaining: 7m 31s\n",
      "370:\tlearn: 0.1270780\ttotal: 4m 26s\tremaining: 7m 31s\n",
      "371:\tlearn: 0.1268276\ttotal: 4m 26s\tremaining: 7m 30s\n",
      "372:\tlearn: 0.1265464\ttotal: 4m 27s\tremaining: 7m 29s\n",
      "373:\tlearn: 0.1262199\ttotal: 4m 28s\tremaining: 7m 28s\n",
      "374:\tlearn: 0.1257835\ttotal: 4m 29s\tremaining: 7m 28s\n",
      "375:\tlearn: 0.1253638\ttotal: 4m 29s\tremaining: 7m 27s\n",
      "376:\tlearn: 0.1251366\ttotal: 4m 30s\tremaining: 7m 27s\n",
      "377:\tlearn: 0.1247559\ttotal: 4m 31s\tremaining: 7m 26s\n",
      "378:\tlearn: 0.1242986\ttotal: 4m 32s\tremaining: 7m 25s\n",
      "379:\tlearn: 0.1239679\ttotal: 4m 32s\tremaining: 7m 25s\n",
      "380:\tlearn: 0.1236311\ttotal: 4m 33s\tremaining: 7m 24s\n",
      "381:\tlearn: 0.1233350\ttotal: 4m 34s\tremaining: 7m 24s\n",
      "382:\tlearn: 0.1228774\ttotal: 4m 36s\tremaining: 7m 24s\n",
      "383:\tlearn: 0.1227096\ttotal: 4m 37s\tremaining: 7m 24s\n",
      "384:\tlearn: 0.1225594\ttotal: 4m 38s\tremaining: 7m 24s\n",
      "385:\tlearn: 0.1222169\ttotal: 4m 39s\tremaining: 7m 24s\n",
      "386:\tlearn: 0.1217390\ttotal: 4m 40s\tremaining: 7m 23s\n",
      "387:\tlearn: 0.1214461\ttotal: 4m 40s\tremaining: 7m 22s\n",
      "388:\tlearn: 0.1212996\ttotal: 4m 41s\tremaining: 7m 22s\n",
      "389:\tlearn: 0.1210541\ttotal: 4m 42s\tremaining: 7m 21s\n",
      "390:\tlearn: 0.1203014\ttotal: 4m 43s\tremaining: 7m 21s\n",
      "391:\tlearn: 0.1200564\ttotal: 4m 44s\tremaining: 7m 20s\n",
      "392:\tlearn: 0.1197957\ttotal: 4m 44s\tremaining: 7m 19s\n",
      "393:\tlearn: 0.1192619\ttotal: 4m 45s\tremaining: 7m 19s\n",
      "394:\tlearn: 0.1191081\ttotal: 4m 46s\tremaining: 7m 18s\n",
      "395:\tlearn: 0.1187368\ttotal: 4m 47s\tremaining: 7m 17s\n",
      "396:\tlearn: 0.1185076\ttotal: 4m 47s\tremaining: 7m 17s\n",
      "397:\tlearn: 0.1184348\ttotal: 4m 48s\tremaining: 7m 16s\n",
      "398:\tlearn: 0.1181141\ttotal: 4m 49s\tremaining: 7m 15s\n",
      "399:\tlearn: 0.1177100\ttotal: 4m 50s\tremaining: 7m 15s\n",
      "400:\tlearn: 0.1172122\ttotal: 4m 50s\tremaining: 7m 14s\n",
      "401:\tlearn: 0.1168972\ttotal: 4m 51s\tremaining: 7m 13s\n",
      "402:\tlearn: 0.1167026\ttotal: 4m 52s\tremaining: 7m 12s\n",
      "403:\tlearn: 0.1165271\ttotal: 4m 52s\tremaining: 7m 12s\n",
      "404:\tlearn: 0.1162506\ttotal: 4m 53s\tremaining: 7m 11s\n",
      "405:\tlearn: 0.1159528\ttotal: 4m 54s\tremaining: 7m 10s\n",
      "406:\tlearn: 0.1157764\ttotal: 4m 55s\tremaining: 7m 9s\n",
      "407:\tlearn: 0.1153057\ttotal: 4m 55s\tremaining: 7m 9s\n",
      "408:\tlearn: 0.1149787\ttotal: 4m 56s\tremaining: 7m 8s\n",
      "409:\tlearn: 0.1147455\ttotal: 4m 57s\tremaining: 7m 7s\n",
      "410:\tlearn: 0.1145407\ttotal: 4m 57s\tremaining: 7m 7s\n",
      "411:\tlearn: 0.1142139\ttotal: 4m 58s\tremaining: 7m 6s\n",
      "412:\tlearn: 0.1140445\ttotal: 4m 59s\tremaining: 7m 5s\n",
      "413:\tlearn: 0.1137197\ttotal: 5m\tremaining: 7m 4s\n",
      "414:\tlearn: 0.1135816\ttotal: 5m\tremaining: 7m 4s\n",
      "415:\tlearn: 0.1133538\ttotal: 5m 1s\tremaining: 7m 3s\n",
      "416:\tlearn: 0.1131893\ttotal: 5m 2s\tremaining: 7m 2s\n",
      "417:\tlearn: 0.1129612\ttotal: 5m 2s\tremaining: 7m 1s\n",
      "418:\tlearn: 0.1126379\ttotal: 5m 3s\tremaining: 7m 1s\n",
      "419:\tlearn: 0.1122194\ttotal: 5m 4s\tremaining: 7m\n",
      "420:\tlearn: 0.1119252\ttotal: 5m 5s\tremaining: 6m 59s\n",
      "421:\tlearn: 0.1114892\ttotal: 5m 5s\tremaining: 6m 58s\n",
      "422:\tlearn: 0.1111798\ttotal: 5m 6s\tremaining: 6m 58s\n",
      "423:\tlearn: 0.1109926\ttotal: 5m 7s\tremaining: 6m 57s\n",
      "424:\tlearn: 0.1107767\ttotal: 5m 7s\tremaining: 6m 56s\n",
      "425:\tlearn: 0.1104611\ttotal: 5m 8s\tremaining: 6m 55s\n",
      "426:\tlearn: 0.1102994\ttotal: 5m 9s\tremaining: 6m 55s\n",
      "427:\tlearn: 0.1101507\ttotal: 5m 10s\tremaining: 6m 54s\n",
      "428:\tlearn: 0.1098215\ttotal: 5m 10s\tremaining: 6m 53s\n",
      "429:\tlearn: 0.1095200\ttotal: 5m 11s\tremaining: 6m 52s\n",
      "430:\tlearn: 0.1092715\ttotal: 5m 12s\tremaining: 6m 52s\n",
      "431:\tlearn: 0.1090947\ttotal: 5m 12s\tremaining: 6m 51s\n",
      "432:\tlearn: 0.1088520\ttotal: 5m 13s\tremaining: 6m 50s\n",
      "433:\tlearn: 0.1084349\ttotal: 5m 14s\tremaining: 6m 49s\n",
      "434:\tlearn: 0.1079621\ttotal: 5m 14s\tremaining: 6m 49s\n",
      "435:\tlearn: 0.1077027\ttotal: 5m 15s\tremaining: 6m 48s\n",
      "436:\tlearn: 0.1073653\ttotal: 5m 16s\tremaining: 6m 47s\n",
      "437:\tlearn: 0.1071179\ttotal: 5m 17s\tremaining: 6m 46s\n",
      "438:\tlearn: 0.1068208\ttotal: 5m 17s\tremaining: 6m 46s\n",
      "439:\tlearn: 0.1065506\ttotal: 5m 18s\tremaining: 6m 45s\n",
      "440:\tlearn: 0.1062587\ttotal: 5m 19s\tremaining: 6m 44s\n",
      "441:\tlearn: 0.1059125\ttotal: 5m 19s\tremaining: 6m 43s\n",
      "442:\tlearn: 0.1056579\ttotal: 5m 20s\tremaining: 6m 43s\n",
      "443:\tlearn: 0.1054562\ttotal: 5m 21s\tremaining: 6m 42s\n",
      "444:\tlearn: 0.1051150\ttotal: 5m 22s\tremaining: 6m 41s\n",
      "445:\tlearn: 0.1048638\ttotal: 5m 22s\tremaining: 6m 41s\n",
      "446:\tlearn: 0.1047455\ttotal: 5m 23s\tremaining: 6m 40s\n",
      "447:\tlearn: 0.1044553\ttotal: 5m 24s\tremaining: 6m 39s\n",
      "448:\tlearn: 0.1042000\ttotal: 5m 24s\tremaining: 6m 38s\n",
      "449:\tlearn: 0.1040889\ttotal: 5m 25s\tremaining: 6m 38s\n",
      "450:\tlearn: 0.1036594\ttotal: 5m 26s\tremaining: 6m 37s\n",
      "451:\tlearn: 0.1033491\ttotal: 5m 27s\tremaining: 6m 36s\n",
      "452:\tlearn: 0.1029614\ttotal: 5m 27s\tremaining: 6m 35s\n",
      "453:\tlearn: 0.1027727\ttotal: 5m 28s\tremaining: 6m 35s\n",
      "454:\tlearn: 0.1025625\ttotal: 5m 29s\tremaining: 6m 34s\n",
      "455:\tlearn: 0.1023260\ttotal: 5m 29s\tremaining: 6m 33s\n",
      "456:\tlearn: 0.1021371\ttotal: 5m 30s\tremaining: 6m 32s\n",
      "457:\tlearn: 0.1018324\ttotal: 5m 31s\tremaining: 6m 32s\n",
      "458:\tlearn: 0.1014898\ttotal: 5m 31s\tremaining: 6m 31s\n",
      "459:\tlearn: 0.1011487\ttotal: 5m 32s\tremaining: 6m 30s\n",
      "460:\tlearn: 0.1008289\ttotal: 5m 33s\tremaining: 6m 29s\n",
      "461:\tlearn: 0.1004800\ttotal: 5m 34s\tremaining: 6m 29s\n",
      "462:\tlearn: 0.1003623\ttotal: 5m 34s\tremaining: 6m 28s\n",
      "463:\tlearn: 0.0999353\ttotal: 5m 35s\tremaining: 6m 27s\n",
      "464:\tlearn: 0.0996061\ttotal: 5m 36s\tremaining: 6m 26s\n",
      "465:\tlearn: 0.0993632\ttotal: 5m 36s\tremaining: 6m 26s\n",
      "466:\tlearn: 0.0990423\ttotal: 5m 37s\tremaining: 6m 25s\n",
      "467:\tlearn: 0.0988996\ttotal: 5m 38s\tremaining: 6m 24s\n",
      "468:\tlearn: 0.0985834\ttotal: 5m 39s\tremaining: 6m 23s\n",
      "469:\tlearn: 0.0982942\ttotal: 5m 39s\tremaining: 6m 23s\n",
      "470:\tlearn: 0.0981315\ttotal: 5m 40s\tremaining: 6m 22s\n",
      "471:\tlearn: 0.0979376\ttotal: 5m 41s\tremaining: 6m 21s\n",
      "472:\tlearn: 0.0977225\ttotal: 5m 41s\tremaining: 6m 20s\n",
      "473:\tlearn: 0.0975733\ttotal: 5m 42s\tremaining: 6m 20s\n",
      "474:\tlearn: 0.0973382\ttotal: 5m 43s\tremaining: 6m 19s\n",
      "475:\tlearn: 0.0971020\ttotal: 5m 43s\tremaining: 6m 18s\n",
      "476:\tlearn: 0.0969640\ttotal: 5m 44s\tremaining: 6m 17s\n",
      "477:\tlearn: 0.0967836\ttotal: 5m 45s\tremaining: 6m 17s\n",
      "478:\tlearn: 0.0966028\ttotal: 5m 46s\tremaining: 6m 16s\n",
      "479:\tlearn: 0.0963942\ttotal: 5m 46s\tremaining: 6m 15s\n",
      "480:\tlearn: 0.0962571\ttotal: 5m 47s\tremaining: 6m 14s\n",
      "481:\tlearn: 0.0959187\ttotal: 5m 48s\tremaining: 6m 14s\n",
      "482:\tlearn: 0.0957562\ttotal: 5m 48s\tremaining: 6m 13s\n",
      "483:\tlearn: 0.0954201\ttotal: 5m 49s\tremaining: 6m 12s\n",
      "484:\tlearn: 0.0951442\ttotal: 5m 50s\tremaining: 6m 11s\n",
      "485:\tlearn: 0.0949559\ttotal: 5m 51s\tremaining: 6m 11s\n",
      "486:\tlearn: 0.0947130\ttotal: 5m 51s\tremaining: 6m 10s\n",
      "487:\tlearn: 0.0945202\ttotal: 5m 52s\tremaining: 6m 9s\n",
      "488:\tlearn: 0.0943898\ttotal: 5m 53s\tremaining: 6m 9s\n",
      "489:\tlearn: 0.0941500\ttotal: 5m 53s\tremaining: 6m 8s\n",
      "490:\tlearn: 0.0940059\ttotal: 5m 54s\tremaining: 6m 7s\n",
      "491:\tlearn: 0.0938886\ttotal: 5m 55s\tremaining: 6m 6s\n",
      "492:\tlearn: 0.0934943\ttotal: 5m 56s\tremaining: 6m 6s\n",
      "493:\tlearn: 0.0932309\ttotal: 5m 56s\tremaining: 6m 5s\n",
      "494:\tlearn: 0.0930092\ttotal: 5m 57s\tremaining: 6m 4s\n",
      "495:\tlearn: 0.0928721\ttotal: 5m 58s\tremaining: 6m 3s\n",
      "496:\tlearn: 0.0927456\ttotal: 5m 58s\tremaining: 6m 3s\n",
      "497:\tlearn: 0.0925012\ttotal: 5m 59s\tremaining: 6m 2s\n",
      "498:\tlearn: 0.0923263\ttotal: 6m\tremaining: 6m 1s\n",
      "499:\tlearn: 0.0921197\ttotal: 6m\tremaining: 6m\n",
      "500:\tlearn: 0.0917033\ttotal: 6m 1s\tremaining: 6m\n",
      "501:\tlearn: 0.0915086\ttotal: 6m 2s\tremaining: 5m 59s\n",
      "502:\tlearn: 0.0913458\ttotal: 6m 3s\tremaining: 5m 58s\n",
      "503:\tlearn: 0.0912095\ttotal: 6m 3s\tremaining: 5m 57s\n",
      "504:\tlearn: 0.0909939\ttotal: 6m 4s\tremaining: 5m 57s\n",
      "505:\tlearn: 0.0908231\ttotal: 6m 5s\tremaining: 5m 56s\n",
      "506:\tlearn: 0.0906679\ttotal: 6m 5s\tremaining: 5m 55s\n",
      "507:\tlearn: 0.0904983\ttotal: 6m 6s\tremaining: 5m 55s\n",
      "508:\tlearn: 0.0902837\ttotal: 6m 7s\tremaining: 5m 54s\n",
      "509:\tlearn: 0.0901357\ttotal: 6m 8s\tremaining: 5m 53s\n",
      "510:\tlearn: 0.0898889\ttotal: 6m 8s\tremaining: 5m 52s\n",
      "511:\tlearn: 0.0896038\ttotal: 6m 9s\tremaining: 5m 52s\n",
      "512:\tlearn: 0.0892280\ttotal: 6m 10s\tremaining: 5m 51s\n",
      "513:\tlearn: 0.0890587\ttotal: 6m 10s\tremaining: 5m 50s\n",
      "514:\tlearn: 0.0889193\ttotal: 6m 11s\tremaining: 5m 49s\n",
      "515:\tlearn: 0.0887335\ttotal: 6m 12s\tremaining: 5m 49s\n",
      "516:\tlearn: 0.0885553\ttotal: 6m 13s\tremaining: 5m 48s\n",
      "517:\tlearn: 0.0882637\ttotal: 6m 13s\tremaining: 5m 47s\n",
      "518:\tlearn: 0.0880349\ttotal: 6m 14s\tremaining: 5m 47s\n",
      "519:\tlearn: 0.0878960\ttotal: 6m 15s\tremaining: 5m 46s\n",
      "520:\tlearn: 0.0877878\ttotal: 6m 15s\tremaining: 5m 45s\n",
      "521:\tlearn: 0.0874690\ttotal: 6m 16s\tremaining: 5m 44s\n",
      "522:\tlearn: 0.0872516\ttotal: 6m 17s\tremaining: 5m 44s\n",
      "523:\tlearn: 0.0870376\ttotal: 6m 17s\tremaining: 5m 43s\n",
      "524:\tlearn: 0.0868812\ttotal: 6m 18s\tremaining: 5m 42s\n",
      "525:\tlearn: 0.0867230\ttotal: 6m 19s\tremaining: 5m 41s\n",
      "526:\tlearn: 0.0864852\ttotal: 6m 20s\tremaining: 5m 41s\n",
      "527:\tlearn: 0.0863296\ttotal: 6m 20s\tremaining: 5m 40s\n",
      "528:\tlearn: 0.0860595\ttotal: 6m 21s\tremaining: 5m 39s\n",
      "529:\tlearn: 0.0859526\ttotal: 6m 22s\tremaining: 5m 38s\n",
      "530:\tlearn: 0.0856841\ttotal: 6m 22s\tremaining: 5m 38s\n",
      "531:\tlearn: 0.0854366\ttotal: 6m 23s\tremaining: 5m 37s\n",
      "532:\tlearn: 0.0852517\ttotal: 6m 24s\tremaining: 5m 36s\n",
      "533:\tlearn: 0.0851388\ttotal: 6m 24s\tremaining: 5m 35s\n",
      "534:\tlearn: 0.0849649\ttotal: 6m 25s\tremaining: 5m 35s\n",
      "535:\tlearn: 0.0848309\ttotal: 6m 26s\tremaining: 5m 34s\n",
      "536:\tlearn: 0.0846176\ttotal: 6m 27s\tremaining: 5m 33s\n",
      "537:\tlearn: 0.0844957\ttotal: 6m 27s\tremaining: 5m 33s\n",
      "538:\tlearn: 0.0843314\ttotal: 6m 28s\tremaining: 5m 32s\n",
      "539:\tlearn: 0.0842263\ttotal: 6m 29s\tremaining: 5m 31s\n",
      "540:\tlearn: 0.0841155\ttotal: 6m 29s\tremaining: 5m 30s\n",
      "541:\tlearn: 0.0839822\ttotal: 6m 30s\tremaining: 5m 30s\n",
      "542:\tlearn: 0.0838339\ttotal: 6m 31s\tremaining: 5m 29s\n",
      "543:\tlearn: 0.0835866\ttotal: 6m 31s\tremaining: 5m 28s\n",
      "544:\tlearn: 0.0833654\ttotal: 6m 32s\tremaining: 5m 27s\n",
      "545:\tlearn: 0.0832215\ttotal: 6m 33s\tremaining: 5m 27s\n",
      "546:\tlearn: 0.0830428\ttotal: 6m 34s\tremaining: 5m 26s\n",
      "547:\tlearn: 0.0828971\ttotal: 6m 34s\tremaining: 5m 25s\n",
      "548:\tlearn: 0.0825896\ttotal: 6m 35s\tremaining: 5m 24s\n",
      "549:\tlearn: 0.0824993\ttotal: 6m 36s\tremaining: 5m 24s\n",
      "550:\tlearn: 0.0823978\ttotal: 6m 36s\tremaining: 5m 23s\n",
      "551:\tlearn: 0.0823023\ttotal: 6m 37s\tremaining: 5m 22s\n",
      "552:\tlearn: 0.0821131\ttotal: 6m 38s\tremaining: 5m 21s\n",
      "553:\tlearn: 0.0817790\ttotal: 6m 38s\tremaining: 5m 21s\n",
      "554:\tlearn: 0.0814891\ttotal: 6m 39s\tremaining: 5m 20s\n",
      "555:\tlearn: 0.0813368\ttotal: 6m 40s\tremaining: 5m 19s\n",
      "556:\tlearn: 0.0810716\ttotal: 6m 41s\tremaining: 5m 19s\n",
      "557:\tlearn: 0.0809175\ttotal: 6m 41s\tremaining: 5m 18s\n",
      "558:\tlearn: 0.0807621\ttotal: 6m 42s\tremaining: 5m 17s\n",
      "559:\tlearn: 0.0806659\ttotal: 6m 43s\tremaining: 5m 16s\n",
      "560:\tlearn: 0.0804988\ttotal: 6m 44s\tremaining: 5m 16s\n",
      "561:\tlearn: 0.0804006\ttotal: 6m 44s\tremaining: 5m 15s\n",
      "562:\tlearn: 0.0802478\ttotal: 6m 45s\tremaining: 5m 14s\n",
      "563:\tlearn: 0.0801506\ttotal: 6m 46s\tremaining: 5m 14s\n",
      "564:\tlearn: 0.0800054\ttotal: 6m 46s\tremaining: 5m 13s\n",
      "565:\tlearn: 0.0797278\ttotal: 6m 47s\tremaining: 5m 12s\n",
      "566:\tlearn: 0.0795765\ttotal: 6m 48s\tremaining: 5m 11s\n",
      "567:\tlearn: 0.0794021\ttotal: 6m 49s\tremaining: 5m 11s\n",
      "568:\tlearn: 0.0791601\ttotal: 6m 49s\tremaining: 5m 10s\n",
      "569:\tlearn: 0.0789628\ttotal: 6m 50s\tremaining: 5m 9s\n",
      "570:\tlearn: 0.0788619\ttotal: 6m 51s\tremaining: 5m 8s\n",
      "571:\tlearn: 0.0787396\ttotal: 6m 51s\tremaining: 5m 8s\n",
      "572:\tlearn: 0.0785186\ttotal: 6m 52s\tremaining: 5m 7s\n",
      "573:\tlearn: 0.0783728\ttotal: 6m 53s\tremaining: 5m 6s\n",
      "574:\tlearn: 0.0781589\ttotal: 6m 53s\tremaining: 5m 5s\n",
      "575:\tlearn: 0.0779102\ttotal: 6m 54s\tremaining: 5m 5s\n",
      "576:\tlearn: 0.0777185\ttotal: 6m 55s\tremaining: 5m 4s\n",
      "577:\tlearn: 0.0776294\ttotal: 6m 56s\tremaining: 5m 3s\n",
      "578:\tlearn: 0.0775459\ttotal: 6m 56s\tremaining: 5m 3s\n",
      "579:\tlearn: 0.0774343\ttotal: 6m 57s\tremaining: 5m 2s\n",
      "580:\tlearn: 0.0772687\ttotal: 6m 58s\tremaining: 5m 1s\n",
      "581:\tlearn: 0.0771760\ttotal: 6m 58s\tremaining: 5m\n",
      "582:\tlearn: 0.0768882\ttotal: 6m 59s\tremaining: 5m\n",
      "583:\tlearn: 0.0766371\ttotal: 7m\tremaining: 4m 59s\n",
      "584:\tlearn: 0.0764698\ttotal: 7m 1s\tremaining: 4m 58s\n",
      "585:\tlearn: 0.0763185\ttotal: 7m 1s\tremaining: 4m 57s\n",
      "586:\tlearn: 0.0760417\ttotal: 7m 2s\tremaining: 4m 57s\n",
      "587:\tlearn: 0.0758589\ttotal: 7m 3s\tremaining: 4m 56s\n",
      "588:\tlearn: 0.0757262\ttotal: 7m 3s\tremaining: 4m 55s\n",
      "589:\tlearn: 0.0756001\ttotal: 7m 4s\tremaining: 4m 54s\n",
      "590:\tlearn: 0.0754234\ttotal: 7m 5s\tremaining: 4m 54s\n",
      "591:\tlearn: 0.0752675\ttotal: 7m 5s\tremaining: 4m 53s\n",
      "592:\tlearn: 0.0751753\ttotal: 7m 6s\tremaining: 4m 52s\n",
      "593:\tlearn: 0.0749007\ttotal: 7m 7s\tremaining: 4m 52s\n",
      "594:\tlearn: 0.0746075\ttotal: 7m 8s\tremaining: 4m 51s\n",
      "595:\tlearn: 0.0744087\ttotal: 7m 8s\tremaining: 4m 50s\n",
      "596:\tlearn: 0.0741396\ttotal: 7m 9s\tremaining: 4m 49s\n",
      "597:\tlearn: 0.0740610\ttotal: 7m 10s\tremaining: 4m 49s\n",
      "598:\tlearn: 0.0738303\ttotal: 7m 10s\tremaining: 4m 48s\n",
      "599:\tlearn: 0.0736290\ttotal: 7m 11s\tremaining: 4m 47s\n",
      "600:\tlearn: 0.0733795\ttotal: 7m 12s\tremaining: 4m 46s\n",
      "601:\tlearn: 0.0732631\ttotal: 7m 12s\tremaining: 4m 46s\n",
      "602:\tlearn: 0.0731685\ttotal: 7m 13s\tremaining: 4m 45s\n",
      "603:\tlearn: 0.0730610\ttotal: 7m 14s\tremaining: 4m 44s\n",
      "604:\tlearn: 0.0729509\ttotal: 7m 15s\tremaining: 4m 44s\n",
      "605:\tlearn: 0.0728034\ttotal: 7m 15s\tremaining: 4m 43s\n",
      "606:\tlearn: 0.0726279\ttotal: 7m 16s\tremaining: 4m 42s\n",
      "607:\tlearn: 0.0724952\ttotal: 7m 17s\tremaining: 4m 41s\n",
      "608:\tlearn: 0.0724013\ttotal: 7m 17s\tremaining: 4m 41s\n",
      "609:\tlearn: 0.0722747\ttotal: 7m 18s\tremaining: 4m 40s\n",
      "610:\tlearn: 0.0720333\ttotal: 7m 19s\tremaining: 4m 39s\n",
      "611:\tlearn: 0.0719705\ttotal: 7m 19s\tremaining: 4m 38s\n",
      "612:\tlearn: 0.0718082\ttotal: 7m 20s\tremaining: 4m 38s\n",
      "613:\tlearn: 0.0717515\ttotal: 7m 21s\tremaining: 4m 37s\n",
      "614:\tlearn: 0.0716565\ttotal: 7m 22s\tremaining: 4m 36s\n",
      "615:\tlearn: 0.0715156\ttotal: 7m 22s\tremaining: 4m 36s\n",
      "616:\tlearn: 0.0713883\ttotal: 7m 23s\tremaining: 4m 35s\n",
      "617:\tlearn: 0.0711997\ttotal: 7m 24s\tremaining: 4m 34s\n",
      "618:\tlearn: 0.0711526\ttotal: 7m 25s\tremaining: 4m 33s\n",
      "619:\tlearn: 0.0709938\ttotal: 7m 25s\tremaining: 4m 33s\n",
      "620:\tlearn: 0.0709130\ttotal: 7m 26s\tremaining: 4m 32s\n",
      "621:\tlearn: 0.0707456\ttotal: 7m 27s\tremaining: 4m 31s\n",
      "622:\tlearn: 0.0706387\ttotal: 7m 27s\tremaining: 4m 31s\n",
      "623:\tlearn: 0.0705640\ttotal: 7m 28s\tremaining: 4m 30s\n",
      "624:\tlearn: 0.0704414\ttotal: 7m 29s\tremaining: 4m 29s\n",
      "625:\tlearn: 0.0702321\ttotal: 7m 30s\tremaining: 4m 28s\n",
      "626:\tlearn: 0.0701304\ttotal: 7m 30s\tremaining: 4m 28s\n",
      "627:\tlearn: 0.0699508\ttotal: 7m 31s\tremaining: 4m 27s\n",
      "628:\tlearn: 0.0698168\ttotal: 7m 32s\tremaining: 4m 26s\n",
      "629:\tlearn: 0.0697530\ttotal: 7m 32s\tremaining: 4m 25s\n",
      "630:\tlearn: 0.0696263\ttotal: 7m 33s\tremaining: 4m 25s\n",
      "631:\tlearn: 0.0695264\ttotal: 7m 34s\tremaining: 4m 24s\n",
      "632:\tlearn: 0.0694533\ttotal: 7m 35s\tremaining: 4m 23s\n",
      "633:\tlearn: 0.0693592\ttotal: 7m 35s\tremaining: 4m 23s\n",
      "634:\tlearn: 0.0692279\ttotal: 7m 36s\tremaining: 4m 22s\n",
      "635:\tlearn: 0.0690624\ttotal: 7m 37s\tremaining: 4m 21s\n",
      "636:\tlearn: 0.0689634\ttotal: 7m 37s\tremaining: 4m 20s\n",
      "637:\tlearn: 0.0688588\ttotal: 7m 38s\tremaining: 4m 20s\n",
      "638:\tlearn: 0.0686889\ttotal: 7m 39s\tremaining: 4m 19s\n",
      "639:\tlearn: 0.0685841\ttotal: 7m 40s\tremaining: 4m 18s\n",
      "640:\tlearn: 0.0684870\ttotal: 7m 41s\tremaining: 4m 18s\n",
      "641:\tlearn: 0.0682434\ttotal: 7m 41s\tremaining: 4m 17s\n",
      "642:\tlearn: 0.0681217\ttotal: 7m 42s\tremaining: 4m 16s\n",
      "643:\tlearn: 0.0680197\ttotal: 7m 43s\tremaining: 4m 16s\n",
      "644:\tlearn: 0.0678509\ttotal: 7m 43s\tremaining: 4m 15s\n",
      "645:\tlearn: 0.0677025\ttotal: 7m 44s\tremaining: 4m 14s\n",
      "646:\tlearn: 0.0675945\ttotal: 7m 45s\tremaining: 4m 13s\n",
      "647:\tlearn: 0.0675282\ttotal: 7m 45s\tremaining: 4m 13s\n",
      "648:\tlearn: 0.0674407\ttotal: 7m 46s\tremaining: 4m 12s\n",
      "649:\tlearn: 0.0673281\ttotal: 7m 47s\tremaining: 4m 11s\n",
      "650:\tlearn: 0.0671954\ttotal: 7m 48s\tremaining: 4m 10s\n",
      "651:\tlearn: 0.0671371\ttotal: 7m 48s\tremaining: 4m 10s\n",
      "652:\tlearn: 0.0670072\ttotal: 7m 49s\tremaining: 4m 9s\n",
      "653:\tlearn: 0.0669897\ttotal: 7m 50s\tremaining: 4m 8s\n",
      "654:\tlearn: 0.0667600\ttotal: 7m 50s\tremaining: 4m 8s\n",
      "655:\tlearn: 0.0666478\ttotal: 7m 51s\tremaining: 4m 7s\n",
      "656:\tlearn: 0.0664517\ttotal: 7m 52s\tremaining: 4m 6s\n",
      "657:\tlearn: 0.0663039\ttotal: 7m 52s\tremaining: 4m 5s\n",
      "658:\tlearn: 0.0661250\ttotal: 7m 53s\tremaining: 4m 5s\n",
      "659:\tlearn: 0.0659711\ttotal: 7m 54s\tremaining: 4m 4s\n",
      "660:\tlearn: 0.0658181\ttotal: 7m 55s\tremaining: 4m 3s\n",
      "661:\tlearn: 0.0657073\ttotal: 7m 55s\tremaining: 4m 2s\n",
      "662:\tlearn: 0.0656130\ttotal: 7m 56s\tremaining: 4m 2s\n",
      "663:\tlearn: 0.0654736\ttotal: 7m 57s\tremaining: 4m 1s\n",
      "664:\tlearn: 0.0652616\ttotal: 7m 57s\tremaining: 4m\n",
      "665:\tlearn: 0.0650444\ttotal: 7m 58s\tremaining: 4m\n",
      "666:\tlearn: 0.0648573\ttotal: 7m 59s\tremaining: 3m 59s\n",
      "667:\tlearn: 0.0646635\ttotal: 8m\tremaining: 3m 58s\n",
      "668:\tlearn: 0.0644799\ttotal: 8m\tremaining: 3m 57s\n",
      "669:\tlearn: 0.0643319\ttotal: 8m 1s\tremaining: 3m 57s\n",
      "670:\tlearn: 0.0642587\ttotal: 8m 2s\tremaining: 3m 56s\n",
      "671:\tlearn: 0.0641306\ttotal: 8m 2s\tremaining: 3m 55s\n",
      "672:\tlearn: 0.0639490\ttotal: 8m 3s\tremaining: 3m 54s\n",
      "673:\tlearn: 0.0638174\ttotal: 8m 4s\tremaining: 3m 54s\n",
      "674:\tlearn: 0.0636855\ttotal: 8m 4s\tremaining: 3m 53s\n",
      "675:\tlearn: 0.0635588\ttotal: 8m 5s\tremaining: 3m 52s\n",
      "676:\tlearn: 0.0634380\ttotal: 8m 6s\tremaining: 3m 52s\n",
      "677:\tlearn: 0.0633253\ttotal: 8m 7s\tremaining: 3m 51s\n",
      "678:\tlearn: 0.0632000\ttotal: 8m 7s\tremaining: 3m 50s\n",
      "679:\tlearn: 0.0631001\ttotal: 8m 8s\tremaining: 3m 49s\n",
      "680:\tlearn: 0.0629895\ttotal: 8m 9s\tremaining: 3m 49s\n",
      "681:\tlearn: 0.0628378\ttotal: 8m 9s\tremaining: 3m 48s\n",
      "682:\tlearn: 0.0627720\ttotal: 8m 10s\tremaining: 3m 47s\n",
      "683:\tlearn: 0.0626774\ttotal: 8m 11s\tremaining: 3m 46s\n",
      "684:\tlearn: 0.0626386\ttotal: 8m 11s\tremaining: 3m 46s\n",
      "685:\tlearn: 0.0623848\ttotal: 8m 12s\tremaining: 3m 45s\n",
      "686:\tlearn: 0.0622674\ttotal: 8m 13s\tremaining: 3m 44s\n",
      "687:\tlearn: 0.0622048\ttotal: 8m 14s\tremaining: 3m 44s\n",
      "688:\tlearn: 0.0621157\ttotal: 8m 14s\tremaining: 3m 43s\n",
      "689:\tlearn: 0.0619964\ttotal: 8m 15s\tremaining: 3m 42s\n",
      "690:\tlearn: 0.0619201\ttotal: 8m 16s\tremaining: 3m 41s\n",
      "691:\tlearn: 0.0617838\ttotal: 8m 16s\tremaining: 3m 41s\n",
      "692:\tlearn: 0.0616074\ttotal: 8m 17s\tremaining: 3m 40s\n",
      "693:\tlearn: 0.0615228\ttotal: 8m 18s\tremaining: 3m 39s\n",
      "694:\tlearn: 0.0613270\ttotal: 8m 18s\tremaining: 3m 38s\n",
      "695:\tlearn: 0.0612873\ttotal: 8m 19s\tremaining: 3m 38s\n",
      "696:\tlearn: 0.0611848\ttotal: 8m 20s\tremaining: 3m 37s\n",
      "697:\tlearn: 0.0610930\ttotal: 8m 21s\tremaining: 3m 36s\n",
      "698:\tlearn: 0.0609041\ttotal: 8m 21s\tremaining: 3m 36s\n",
      "699:\tlearn: 0.0607131\ttotal: 8m 22s\tremaining: 3m 35s\n",
      "700:\tlearn: 0.0605095\ttotal: 8m 23s\tremaining: 3m 34s\n",
      "701:\tlearn: 0.0603820\ttotal: 8m 23s\tremaining: 3m 33s\n",
      "702:\tlearn: 0.0602467\ttotal: 8m 24s\tremaining: 3m 33s\n",
      "703:\tlearn: 0.0601967\ttotal: 8m 25s\tremaining: 3m 32s\n",
      "704:\tlearn: 0.0600391\ttotal: 8m 25s\tremaining: 3m 31s\n",
      "705:\tlearn: 0.0598767\ttotal: 8m 26s\tremaining: 3m 31s\n",
      "706:\tlearn: 0.0597629\ttotal: 8m 27s\tremaining: 3m 30s\n",
      "707:\tlearn: 0.0596422\ttotal: 8m 28s\tremaining: 3m 29s\n",
      "708:\tlearn: 0.0594701\ttotal: 8m 28s\tremaining: 3m 28s\n",
      "709:\tlearn: 0.0593756\ttotal: 8m 29s\tremaining: 3m 28s\n",
      "710:\tlearn: 0.0591768\ttotal: 8m 30s\tremaining: 3m 27s\n",
      "711:\tlearn: 0.0591138\ttotal: 8m 30s\tremaining: 3m 26s\n",
      "712:\tlearn: 0.0590560\ttotal: 8m 31s\tremaining: 3m 25s\n",
      "713:\tlearn: 0.0589905\ttotal: 8m 32s\tremaining: 3m 25s\n",
      "714:\tlearn: 0.0588667\ttotal: 8m 33s\tremaining: 3m 24s\n",
      "715:\tlearn: 0.0587066\ttotal: 8m 34s\tremaining: 3m 24s\n",
      "716:\tlearn: 0.0585824\ttotal: 8m 35s\tremaining: 3m 23s\n",
      "717:\tlearn: 0.0584397\ttotal: 8m 35s\tremaining: 3m 22s\n",
      "718:\tlearn: 0.0582986\ttotal: 8m 36s\tremaining: 3m 21s\n",
      "719:\tlearn: 0.0581720\ttotal: 8m 37s\tremaining: 3m 21s\n",
      "720:\tlearn: 0.0580653\ttotal: 8m 37s\tremaining: 3m 20s\n",
      "721:\tlearn: 0.0579754\ttotal: 8m 38s\tremaining: 3m 19s\n",
      "722:\tlearn: 0.0578402\ttotal: 8m 39s\tremaining: 3m 18s\n",
      "723:\tlearn: 0.0577596\ttotal: 8m 39s\tremaining: 3m 18s\n",
      "724:\tlearn: 0.0575777\ttotal: 8m 40s\tremaining: 3m 17s\n",
      "725:\tlearn: 0.0574458\ttotal: 8m 41s\tremaining: 3m 16s\n",
      "726:\tlearn: 0.0573440\ttotal: 8m 41s\tremaining: 3m 16s\n",
      "727:\tlearn: 0.0571824\ttotal: 8m 42s\tremaining: 3m 15s\n",
      "728:\tlearn: 0.0571044\ttotal: 8m 43s\tremaining: 3m 14s\n",
      "729:\tlearn: 0.0570058\ttotal: 8m 43s\tremaining: 3m 13s\n",
      "730:\tlearn: 0.0568868\ttotal: 8m 44s\tremaining: 3m 13s\n",
      "731:\tlearn: 0.0567437\ttotal: 8m 45s\tremaining: 3m 12s\n",
      "732:\tlearn: 0.0567344\ttotal: 8m 45s\tremaining: 3m 11s\n",
      "733:\tlearn: 0.0566008\ttotal: 8m 46s\tremaining: 3m 10s\n",
      "734:\tlearn: 0.0565617\ttotal: 8m 47s\tremaining: 3m 10s\n",
      "735:\tlearn: 0.0564623\ttotal: 8m 47s\tremaining: 3m 9s\n",
      "736:\tlearn: 0.0563760\ttotal: 8m 48s\tremaining: 3m 8s\n",
      "737:\tlearn: 0.0563363\ttotal: 8m 49s\tremaining: 3m 7s\n",
      "738:\tlearn: 0.0562464\ttotal: 8m 49s\tremaining: 3m 7s\n",
      "739:\tlearn: 0.0561045\ttotal: 8m 50s\tremaining: 3m 6s\n",
      "740:\tlearn: 0.0559818\ttotal: 8m 51s\tremaining: 3m 5s\n",
      "741:\tlearn: 0.0558826\ttotal: 8m 51s\tremaining: 3m 4s\n",
      "742:\tlearn: 0.0558364\ttotal: 8m 52s\tremaining: 3m 4s\n",
      "743:\tlearn: 0.0557539\ttotal: 8m 53s\tremaining: 3m 3s\n",
      "744:\tlearn: 0.0556792\ttotal: 8m 53s\tremaining: 3m 2s\n",
      "745:\tlearn: 0.0555889\ttotal: 8m 54s\tremaining: 3m 1s\n",
      "746:\tlearn: 0.0555239\ttotal: 8m 54s\tremaining: 3m 1s\n",
      "747:\tlearn: 0.0554178\ttotal: 8m 55s\tremaining: 3m\n",
      "748:\tlearn: 0.0553334\ttotal: 8m 56s\tremaining: 2m 59s\n",
      "749:\tlearn: 0.0551902\ttotal: 8m 56s\tremaining: 2m 58s\n",
      "750:\tlearn: 0.0551050\ttotal: 8m 57s\tremaining: 2m 58s\n",
      "751:\tlearn: 0.0550671\ttotal: 8m 58s\tremaining: 2m 57s\n",
      "752:\tlearn: 0.0549596\ttotal: 8m 58s\tremaining: 2m 56s\n",
      "753:\tlearn: 0.0548745\ttotal: 8m 59s\tremaining: 2m 55s\n",
      "754:\tlearn: 0.0547527\ttotal: 9m\tremaining: 2m 55s\n",
      "755:\tlearn: 0.0546829\ttotal: 9m\tremaining: 2m 54s\n",
      "756:\tlearn: 0.0546422\ttotal: 9m 1s\tremaining: 2m 53s\n",
      "757:\tlearn: 0.0544791\ttotal: 9m 2s\tremaining: 2m 53s\n",
      "758:\tlearn: 0.0543446\ttotal: 9m 2s\tremaining: 2m 52s\n",
      "759:\tlearn: 0.0542781\ttotal: 9m 3s\tremaining: 2m 51s\n",
      "760:\tlearn: 0.0542265\ttotal: 9m 3s\tremaining: 2m 50s\n",
      "761:\tlearn: 0.0541464\ttotal: 9m 4s\tremaining: 2m 50s\n",
      "762:\tlearn: 0.0540692\ttotal: 9m 5s\tremaining: 2m 49s\n",
      "763:\tlearn: 0.0539224\ttotal: 9m 5s\tremaining: 2m 48s\n",
      "764:\tlearn: 0.0537694\ttotal: 9m 6s\tremaining: 2m 47s\n",
      "765:\tlearn: 0.0536664\ttotal: 9m 7s\tremaining: 2m 47s\n",
      "766:\tlearn: 0.0535676\ttotal: 9m 7s\tremaining: 2m 46s\n",
      "767:\tlearn: 0.0534653\ttotal: 9m 8s\tremaining: 2m 45s\n",
      "768:\tlearn: 0.0533350\ttotal: 9m 9s\tremaining: 2m 44s\n",
      "769:\tlearn: 0.0531745\ttotal: 9m 9s\tremaining: 2m 44s\n",
      "770:\tlearn: 0.0530788\ttotal: 9m 10s\tremaining: 2m 43s\n",
      "771:\tlearn: 0.0530340\ttotal: 9m 11s\tremaining: 2m 42s\n",
      "772:\tlearn: 0.0529645\ttotal: 9m 11s\tremaining: 2m 42s\n",
      "773:\tlearn: 0.0528541\ttotal: 9m 12s\tremaining: 2m 41s\n",
      "774:\tlearn: 0.0527981\ttotal: 9m 13s\tremaining: 2m 40s\n",
      "775:\tlearn: 0.0526315\ttotal: 9m 13s\tremaining: 2m 39s\n",
      "776:\tlearn: 0.0524601\ttotal: 9m 14s\tremaining: 2m 39s\n",
      "777:\tlearn: 0.0523571\ttotal: 9m 15s\tremaining: 2m 38s\n",
      "778:\tlearn: 0.0522671\ttotal: 9m 15s\tremaining: 2m 37s\n",
      "779:\tlearn: 0.0521252\ttotal: 9m 16s\tremaining: 2m 36s\n",
      "780:\tlearn: 0.0520022\ttotal: 9m 17s\tremaining: 2m 36s\n",
      "781:\tlearn: 0.0518924\ttotal: 9m 17s\tremaining: 2m 35s\n",
      "782:\tlearn: 0.0517936\ttotal: 9m 18s\tremaining: 2m 34s\n",
      "783:\tlearn: 0.0517102\ttotal: 9m 19s\tremaining: 2m 34s\n",
      "784:\tlearn: 0.0515898\ttotal: 9m 19s\tremaining: 2m 33s\n",
      "785:\tlearn: 0.0515056\ttotal: 9m 20s\tremaining: 2m 32s\n",
      "786:\tlearn: 0.0514497\ttotal: 9m 21s\tremaining: 2m 31s\n",
      "787:\tlearn: 0.0513572\ttotal: 9m 21s\tremaining: 2m 31s\n",
      "788:\tlearn: 0.0512751\ttotal: 9m 22s\tremaining: 2m 30s\n",
      "789:\tlearn: 0.0511325\ttotal: 9m 22s\tremaining: 2m 29s\n",
      "790:\tlearn: 0.0510180\ttotal: 9m 23s\tremaining: 2m 28s\n",
      "791:\tlearn: 0.0509250\ttotal: 9m 24s\tremaining: 2m 28s\n",
      "792:\tlearn: 0.0508225\ttotal: 9m 24s\tremaining: 2m 27s\n",
      "793:\tlearn: 0.0507220\ttotal: 9m 25s\tremaining: 2m 26s\n",
      "794:\tlearn: 0.0506599\ttotal: 9m 26s\tremaining: 2m 25s\n",
      "795:\tlearn: 0.0505788\ttotal: 9m 26s\tremaining: 2m 25s\n",
      "796:\tlearn: 0.0505269\ttotal: 9m 27s\tremaining: 2m 24s\n",
      "797:\tlearn: 0.0504783\ttotal: 9m 28s\tremaining: 2m 23s\n",
      "798:\tlearn: 0.0504388\ttotal: 9m 28s\tremaining: 2m 23s\n",
      "799:\tlearn: 0.0503392\ttotal: 9m 29s\tremaining: 2m 22s\n",
      "800:\tlearn: 0.0502260\ttotal: 9m 30s\tremaining: 2m 21s\n",
      "801:\tlearn: 0.0501806\ttotal: 9m 30s\tremaining: 2m 20s\n",
      "802:\tlearn: 0.0501120\ttotal: 9m 31s\tremaining: 2m 20s\n",
      "803:\tlearn: 0.0500459\ttotal: 9m 31s\tremaining: 2m 19s\n",
      "804:\tlearn: 0.0499958\ttotal: 9m 32s\tremaining: 2m 18s\n",
      "805:\tlearn: 0.0499102\ttotal: 9m 33s\tremaining: 2m 18s\n",
      "806:\tlearn: 0.0497885\ttotal: 9m 33s\tremaining: 2m 17s\n",
      "807:\tlearn: 0.0497356\ttotal: 9m 34s\tremaining: 2m 16s\n",
      "808:\tlearn: 0.0496750\ttotal: 9m 35s\tremaining: 2m 15s\n",
      "809:\tlearn: 0.0496326\ttotal: 9m 35s\tremaining: 2m 15s\n",
      "810:\tlearn: 0.0495451\ttotal: 9m 36s\tremaining: 2m 14s\n",
      "811:\tlearn: 0.0494468\ttotal: 9m 37s\tremaining: 2m 13s\n",
      "812:\tlearn: 0.0494173\ttotal: 9m 37s\tremaining: 2m 12s\n",
      "813:\tlearn: 0.0493365\ttotal: 9m 38s\tremaining: 2m 12s\n",
      "814:\tlearn: 0.0492780\ttotal: 9m 39s\tremaining: 2m 11s\n",
      "815:\tlearn: 0.0491486\ttotal: 9m 39s\tremaining: 2m 10s\n",
      "816:\tlearn: 0.0490703\ttotal: 9m 40s\tremaining: 2m 9s\n",
      "817:\tlearn: 0.0489808\ttotal: 9m 40s\tremaining: 2m 9s\n",
      "818:\tlearn: 0.0488799\ttotal: 9m 41s\tremaining: 2m 8s\n",
      "819:\tlearn: 0.0487880\ttotal: 9m 42s\tremaining: 2m 7s\n",
      "820:\tlearn: 0.0487228\ttotal: 9m 42s\tremaining: 2m 7s\n",
      "821:\tlearn: 0.0486163\ttotal: 9m 43s\tremaining: 2m 6s\n",
      "822:\tlearn: 0.0485644\ttotal: 9m 44s\tremaining: 2m 5s\n",
      "823:\tlearn: 0.0484555\ttotal: 9m 44s\tremaining: 2m 4s\n",
      "824:\tlearn: 0.0483627\ttotal: 9m 45s\tremaining: 2m 4s\n",
      "825:\tlearn: 0.0482545\ttotal: 9m 46s\tremaining: 2m 3s\n",
      "826:\tlearn: 0.0480962\ttotal: 9m 46s\tremaining: 2m 2s\n",
      "827:\tlearn: 0.0480390\ttotal: 9m 47s\tremaining: 2m 2s\n",
      "828:\tlearn: 0.0479299\ttotal: 9m 48s\tremaining: 2m 1s\n",
      "829:\tlearn: 0.0478101\ttotal: 9m 48s\tremaining: 2m\n",
      "830:\tlearn: 0.0477461\ttotal: 9m 49s\tremaining: 1m 59s\n",
      "831:\tlearn: 0.0475977\ttotal: 9m 49s\tremaining: 1m 59s\n",
      "832:\tlearn: 0.0475433\ttotal: 9m 50s\tremaining: 1m 58s\n",
      "833:\tlearn: 0.0475132\ttotal: 9m 51s\tremaining: 1m 57s\n",
      "834:\tlearn: 0.0473773\ttotal: 9m 51s\tremaining: 1m 56s\n",
      "835:\tlearn: 0.0472799\ttotal: 9m 52s\tremaining: 1m 56s\n",
      "836:\tlearn: 0.0471640\ttotal: 9m 53s\tremaining: 1m 55s\n",
      "837:\tlearn: 0.0471043\ttotal: 9m 53s\tremaining: 1m 54s\n",
      "838:\tlearn: 0.0470177\ttotal: 9m 54s\tremaining: 1m 54s\n",
      "839:\tlearn: 0.0469697\ttotal: 9m 55s\tremaining: 1m 53s\n",
      "840:\tlearn: 0.0468561\ttotal: 9m 55s\tremaining: 1m 52s\n",
      "841:\tlearn: 0.0467832\ttotal: 9m 56s\tremaining: 1m 51s\n",
      "842:\tlearn: 0.0467015\ttotal: 9m 57s\tremaining: 1m 51s\n",
      "843:\tlearn: 0.0466187\ttotal: 9m 57s\tremaining: 1m 50s\n",
      "844:\tlearn: 0.0465868\ttotal: 9m 58s\tremaining: 1m 49s\n",
      "845:\tlearn: 0.0465319\ttotal: 9m 58s\tremaining: 1m 49s\n",
      "846:\tlearn: 0.0464836\ttotal: 9m 59s\tremaining: 1m 48s\n",
      "847:\tlearn: 0.0464033\ttotal: 10m\tremaining: 1m 47s\n",
      "848:\tlearn: 0.0463126\ttotal: 10m\tremaining: 1m 46s\n",
      "849:\tlearn: 0.0462588\ttotal: 10m 1s\tremaining: 1m 46s\n",
      "850:\tlearn: 0.0461770\ttotal: 10m 2s\tremaining: 1m 45s\n",
      "851:\tlearn: 0.0461264\ttotal: 10m 2s\tremaining: 1m 44s\n",
      "852:\tlearn: 0.0459991\ttotal: 10m 3s\tremaining: 1m 43s\n",
      "853:\tlearn: 0.0458470\ttotal: 10m 4s\tremaining: 1m 43s\n",
      "854:\tlearn: 0.0457831\ttotal: 10m 4s\tremaining: 1m 42s\n",
      "855:\tlearn: 0.0456754\ttotal: 10m 5s\tremaining: 1m 41s\n",
      "856:\tlearn: 0.0455484\ttotal: 10m 6s\tremaining: 1m 41s\n",
      "857:\tlearn: 0.0454387\ttotal: 10m 6s\tremaining: 1m 40s\n",
      "858:\tlearn: 0.0453460\ttotal: 10m 7s\tremaining: 1m 39s\n",
      "859:\tlearn: 0.0453096\ttotal: 10m 7s\tremaining: 1m 38s\n",
      "860:\tlearn: 0.0452835\ttotal: 10m 8s\tremaining: 1m 38s\n",
      "861:\tlearn: 0.0452276\ttotal: 10m 9s\tremaining: 1m 37s\n",
      "862:\tlearn: 0.0451804\ttotal: 10m 9s\tremaining: 1m 36s\n",
      "863:\tlearn: 0.0450805\ttotal: 10m 10s\tremaining: 1m 36s\n",
      "864:\tlearn: 0.0449766\ttotal: 10m 11s\tremaining: 1m 35s\n",
      "865:\tlearn: 0.0449480\ttotal: 10m 11s\tremaining: 1m 34s\n",
      "866:\tlearn: 0.0448531\ttotal: 10m 12s\tremaining: 1m 33s\n",
      "867:\tlearn: 0.0447978\ttotal: 10m 13s\tremaining: 1m 33s\n",
      "868:\tlearn: 0.0447117\ttotal: 10m 13s\tremaining: 1m 32s\n",
      "869:\tlearn: 0.0446424\ttotal: 10m 14s\tremaining: 1m 31s\n",
      "870:\tlearn: 0.0445258\ttotal: 10m 15s\tremaining: 1m 31s\n",
      "871:\tlearn: 0.0445018\ttotal: 10m 15s\tremaining: 1m 30s\n",
      "872:\tlearn: 0.0444363\ttotal: 10m 16s\tremaining: 1m 29s\n",
      "873:\tlearn: 0.0443072\ttotal: 10m 17s\tremaining: 1m 28s\n",
      "874:\tlearn: 0.0442201\ttotal: 10m 17s\tremaining: 1m 28s\n",
      "875:\tlearn: 0.0442041\ttotal: 10m 18s\tremaining: 1m 27s\n",
      "876:\tlearn: 0.0441365\ttotal: 10m 19s\tremaining: 1m 26s\n",
      "877:\tlearn: 0.0441128\ttotal: 10m 19s\tremaining: 1m 26s\n",
      "878:\tlearn: 0.0440616\ttotal: 10m 20s\tremaining: 1m 25s\n",
      "879:\tlearn: 0.0440011\ttotal: 10m 21s\tremaining: 1m 24s\n",
      "880:\tlearn: 0.0439626\ttotal: 10m 21s\tremaining: 1m 23s\n",
      "881:\tlearn: 0.0438485\ttotal: 10m 22s\tremaining: 1m 23s\n",
      "882:\tlearn: 0.0437803\ttotal: 10m 23s\tremaining: 1m 22s\n",
      "883:\tlearn: 0.0436633\ttotal: 10m 23s\tremaining: 1m 21s\n",
      "884:\tlearn: 0.0436238\ttotal: 10m 24s\tremaining: 1m 21s\n",
      "885:\tlearn: 0.0435791\ttotal: 10m 25s\tremaining: 1m 20s\n",
      "886:\tlearn: 0.0435099\ttotal: 10m 25s\tremaining: 1m 19s\n",
      "887:\tlearn: 0.0434293\ttotal: 10m 26s\tremaining: 1m 19s\n",
      "888:\tlearn: 0.0433816\ttotal: 10m 27s\tremaining: 1m 18s\n",
      "889:\tlearn: 0.0433194\ttotal: 10m 27s\tremaining: 1m 17s\n",
      "890:\tlearn: 0.0432570\ttotal: 10m 28s\tremaining: 1m 16s\n",
      "891:\tlearn: 0.0432080\ttotal: 10m 28s\tremaining: 1m 16s\n",
      "892:\tlearn: 0.0431420\ttotal: 10m 29s\tremaining: 1m 15s\n",
      "893:\tlearn: 0.0430641\ttotal: 10m 30s\tremaining: 1m 14s\n",
      "894:\tlearn: 0.0430060\ttotal: 10m 31s\tremaining: 1m 14s\n",
      "895:\tlearn: 0.0429251\ttotal: 10m 32s\tremaining: 1m 13s\n",
      "896:\tlearn: 0.0428920\ttotal: 10m 33s\tremaining: 1m 12s\n",
      "897:\tlearn: 0.0428469\ttotal: 10m 34s\tremaining: 1m 12s\n",
      "898:\tlearn: 0.0427516\ttotal: 10m 35s\tremaining: 1m 11s\n",
      "899:\tlearn: 0.0426546\ttotal: 10m 35s\tremaining: 1m 10s\n",
      "900:\tlearn: 0.0426148\ttotal: 10m 36s\tremaining: 1m 9s\n",
      "901:\tlearn: 0.0425566\ttotal: 10m 37s\tremaining: 1m 9s\n",
      "902:\tlearn: 0.0424598\ttotal: 10m 38s\tremaining: 1m 8s\n",
      "903:\tlearn: 0.0423667\ttotal: 10m 39s\tremaining: 1m 7s\n",
      "904:\tlearn: 0.0422946\ttotal: 10m 40s\tremaining: 1m 7s\n",
      "905:\tlearn: 0.0422154\ttotal: 10m 41s\tremaining: 1m 6s\n",
      "906:\tlearn: 0.0421926\ttotal: 10m 41s\tremaining: 1m 5s\n",
      "907:\tlearn: 0.0421349\ttotal: 10m 42s\tremaining: 1m 5s\n",
      "908:\tlearn: 0.0420641\ttotal: 10m 43s\tremaining: 1m 4s\n",
      "909:\tlearn: 0.0420115\ttotal: 10m 44s\tremaining: 1m 3s\n",
      "910:\tlearn: 0.0419630\ttotal: 10m 45s\tremaining: 1m 3s\n",
      "911:\tlearn: 0.0419239\ttotal: 10m 46s\tremaining: 1m 2s\n",
      "912:\tlearn: 0.0418385\ttotal: 10m 46s\tremaining: 1m 1s\n",
      "913:\tlearn: 0.0417679\ttotal: 10m 47s\tremaining: 1m\n",
      "914:\tlearn: 0.0417082\ttotal: 10m 48s\tremaining: 1m\n",
      "915:\tlearn: 0.0416569\ttotal: 10m 49s\tremaining: 59.5s\n",
      "916:\tlearn: 0.0416136\ttotal: 10m 50s\tremaining: 58.8s\n",
      "917:\tlearn: 0.0415212\ttotal: 10m 51s\tremaining: 58.2s\n",
      "918:\tlearn: 0.0414209\ttotal: 10m 52s\tremaining: 57.5s\n",
      "919:\tlearn: 0.0413890\ttotal: 10m 52s\tremaining: 56.8s\n",
      "920:\tlearn: 0.0413578\ttotal: 10m 53s\tremaining: 56.1s\n",
      "921:\tlearn: 0.0412633\ttotal: 10m 54s\tremaining: 55.4s\n",
      "922:\tlearn: 0.0412095\ttotal: 10m 55s\tremaining: 54.7s\n",
      "923:\tlearn: 0.0411498\ttotal: 10m 56s\tremaining: 54s\n",
      "924:\tlearn: 0.0411209\ttotal: 10m 57s\tremaining: 53.3s\n",
      "925:\tlearn: 0.0410412\ttotal: 10m 58s\tremaining: 52.6s\n",
      "926:\tlearn: 0.0410102\ttotal: 10m 59s\tremaining: 51.9s\n",
      "927:\tlearn: 0.0408984\ttotal: 10m 59s\tremaining: 51.2s\n",
      "928:\tlearn: 0.0408595\ttotal: 11m\tremaining: 50.5s\n",
      "929:\tlearn: 0.0407931\ttotal: 11m 1s\tremaining: 49.8s\n",
      "930:\tlearn: 0.0407692\ttotal: 11m 1s\tremaining: 49.1s\n",
      "931:\tlearn: 0.0406854\ttotal: 11m 2s\tremaining: 48.3s\n",
      "932:\tlearn: 0.0405963\ttotal: 11m 3s\tremaining: 47.6s\n",
      "933:\tlearn: 0.0405428\ttotal: 11m 4s\tremaining: 46.9s\n",
      "934:\tlearn: 0.0404824\ttotal: 11m 4s\tremaining: 46.2s\n",
      "935:\tlearn: 0.0404090\ttotal: 11m 5s\tremaining: 45.5s\n",
      "936:\tlearn: 0.0403689\ttotal: 11m 6s\tremaining: 44.8s\n",
      "937:\tlearn: 0.0403175\ttotal: 11m 6s\tremaining: 44.1s\n",
      "938:\tlearn: 0.0402294\ttotal: 11m 7s\tremaining: 43.4s\n",
      "939:\tlearn: 0.0401564\ttotal: 11m 8s\tremaining: 42.7s\n",
      "940:\tlearn: 0.0400962\ttotal: 11m 8s\tremaining: 41.9s\n",
      "941:\tlearn: 0.0400336\ttotal: 11m 9s\tremaining: 41.2s\n",
      "942:\tlearn: 0.0399973\ttotal: 11m 10s\tremaining: 40.5s\n",
      "943:\tlearn: 0.0399572\ttotal: 11m 11s\tremaining: 39.8s\n",
      "944:\tlearn: 0.0399222\ttotal: 11m 11s\tremaining: 39.1s\n",
      "945:\tlearn: 0.0398600\ttotal: 11m 12s\tremaining: 38.4s\n",
      "946:\tlearn: 0.0397846\ttotal: 11m 13s\tremaining: 37.7s\n",
      "947:\tlearn: 0.0397541\ttotal: 11m 13s\tremaining: 37s\n",
      "948:\tlearn: 0.0396884\ttotal: 11m 14s\tremaining: 36.3s\n",
      "949:\tlearn: 0.0396430\ttotal: 11m 15s\tremaining: 35.5s\n",
      "950:\tlearn: 0.0395858\ttotal: 11m 16s\tremaining: 34.8s\n",
      "951:\tlearn: 0.0395558\ttotal: 11m 16s\tremaining: 34.1s\n",
      "952:\tlearn: 0.0394886\ttotal: 11m 17s\tremaining: 33.4s\n",
      "953:\tlearn: 0.0394322\ttotal: 11m 18s\tremaining: 32.7s\n",
      "954:\tlearn: 0.0393760\ttotal: 11m 18s\tremaining: 32s\n",
      "955:\tlearn: 0.0393392\ttotal: 11m 19s\tremaining: 31.3s\n",
      "956:\tlearn: 0.0393170\ttotal: 11m 20s\tremaining: 30.6s\n",
      "957:\tlearn: 0.0392626\ttotal: 11m 20s\tremaining: 29.9s\n",
      "958:\tlearn: 0.0391640\ttotal: 11m 21s\tremaining: 29.1s\n",
      "959:\tlearn: 0.0390420\ttotal: 11m 22s\tremaining: 28.4s\n",
      "960:\tlearn: 0.0389928\ttotal: 11m 23s\tremaining: 27.7s\n",
      "961:\tlearn: 0.0389486\ttotal: 11m 23s\tremaining: 27s\n",
      "962:\tlearn: 0.0388627\ttotal: 11m 24s\tremaining: 26.3s\n",
      "963:\tlearn: 0.0387988\ttotal: 11m 25s\tremaining: 25.6s\n",
      "964:\tlearn: 0.0387333\ttotal: 11m 25s\tremaining: 24.9s\n",
      "965:\tlearn: 0.0386791\ttotal: 11m 26s\tremaining: 24.2s\n",
      "966:\tlearn: 0.0386084\ttotal: 11m 27s\tremaining: 23.5s\n",
      "967:\tlearn: 0.0385306\ttotal: 11m 27s\tremaining: 22.7s\n",
      "968:\tlearn: 0.0384780\ttotal: 11m 28s\tremaining: 22s\n",
      "969:\tlearn: 0.0384141\ttotal: 11m 29s\tremaining: 21.3s\n",
      "970:\tlearn: 0.0383488\ttotal: 11m 30s\tremaining: 20.6s\n",
      "971:\tlearn: 0.0383043\ttotal: 11m 30s\tremaining: 19.9s\n",
      "972:\tlearn: 0.0382334\ttotal: 11m 31s\tremaining: 19.2s\n",
      "973:\tlearn: 0.0382115\ttotal: 11m 32s\tremaining: 18.5s\n",
      "974:\tlearn: 0.0381321\ttotal: 11m 33s\tremaining: 17.8s\n",
      "975:\tlearn: 0.0381105\ttotal: 11m 33s\tremaining: 17.1s\n",
      "976:\tlearn: 0.0380630\ttotal: 11m 34s\tremaining: 16.3s\n",
      "977:\tlearn: 0.0380346\ttotal: 11m 35s\tremaining: 15.6s\n",
      "978:\tlearn: 0.0379720\ttotal: 11m 35s\tremaining: 14.9s\n",
      "979:\tlearn: 0.0379182\ttotal: 11m 36s\tremaining: 14.2s\n",
      "980:\tlearn: 0.0378752\ttotal: 11m 37s\tremaining: 13.5s\n",
      "981:\tlearn: 0.0378367\ttotal: 11m 37s\tremaining: 12.8s\n",
      "982:\tlearn: 0.0377365\ttotal: 11m 38s\tremaining: 12.1s\n",
      "983:\tlearn: 0.0377058\ttotal: 11m 39s\tremaining: 11.4s\n",
      "984:\tlearn: 0.0376482\ttotal: 11m 40s\tremaining: 10.7s\n",
      "985:\tlearn: 0.0375982\ttotal: 11m 40s\tremaining: 9.95s\n",
      "986:\tlearn: 0.0375264\ttotal: 11m 41s\tremaining: 9.24s\n",
      "987:\tlearn: 0.0374588\ttotal: 11m 42s\tremaining: 8.53s\n",
      "988:\tlearn: 0.0374178\ttotal: 11m 42s\tremaining: 7.82s\n",
      "989:\tlearn: 0.0373356\ttotal: 11m 43s\tremaining: 7.11s\n",
      "990:\tlearn: 0.0373105\ttotal: 11m 44s\tremaining: 6.39s\n",
      "991:\tlearn: 0.0372706\ttotal: 11m 44s\tremaining: 5.68s\n",
      "992:\tlearn: 0.0372434\ttotal: 11m 45s\tremaining: 4.97s\n",
      "993:\tlearn: 0.0371701\ttotal: 11m 46s\tremaining: 4.26s\n",
      "994:\tlearn: 0.0371583\ttotal: 11m 47s\tremaining: 3.55s\n",
      "995:\tlearn: 0.0371061\ttotal: 11m 47s\tremaining: 2.84s\n",
      "996:\tlearn: 0.0370535\ttotal: 11m 48s\tremaining: 2.13s\n",
      "997:\tlearn: 0.0369872\ttotal: 11m 49s\tremaining: 1.42s\n",
      "998:\tlearn: 0.0369649\ttotal: 11m 49s\tremaining: 711ms\n",
      "999:\tlearn: 0.0369208\ttotal: 11m 50s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "clf11 = CatBoostClassifier()\n",
    "clf11.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred = clf11.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90        77\n",
      "           1       0.96      0.93      0.95        58\n",
      "           2       0.93      1.00      0.96        53\n",
      "           3       0.77      0.82      0.79        60\n",
      "           4       0.90      0.79      0.84        76\n",
      "           5       0.97      0.93      0.95        60\n",
      "           6       0.78      0.96      0.86        49\n",
      "           7       0.72      0.86      0.78        56\n",
      "           8       0.75      0.82      0.78        49\n",
      "           9       0.96      0.74      0.84        62\n",
      "\n",
      "    accuracy                           0.86       600\n",
      "   macro avg       0.87      0.87      0.87       600\n",
      "weighted avg       0.87      0.86      0.87       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier, LGBMClassifier, and CatBoostClassifier show good performance.\n",
    "These are based on the gradient boosting algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
